{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логістична регресія "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import (get_x, get_y, tokenize_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'data.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenize_x(get_x(df))\n",
    "y = get_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float).view(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "input_dim = 50\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "     def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim=input_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренування"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000, Training Loss: 64.33207702636719, Validation Loss: 62.55073165893555\n",
      "Epoch: 2/1000, Training Loss: 62.329071044921875, Validation Loss: 61.70651626586914\n",
      "Epoch: 3/1000, Training Loss: 61.4993896484375, Validation Loss: 60.452850341796875\n",
      "Epoch: 4/1000, Training Loss: 60.362857818603516, Validation Loss: 59.59870529174805\n",
      "Epoch: 5/1000, Training Loss: 59.40127944946289, Validation Loss: 58.10063552856445\n",
      "Epoch: 6/1000, Training Loss: 57.87353515625, Validation Loss: 56.708160400390625\n",
      "Epoch: 7/1000, Training Loss: 56.58474349975586, Validation Loss: 55.08082580566406\n",
      "Epoch: 8/1000, Training Loss: 55.06804656982422, Validation Loss: 53.58285903930664\n",
      "Epoch: 9/1000, Training Loss: 53.52397918701172, Validation Loss: 51.767669677734375\n",
      "Epoch: 10/1000, Training Loss: 51.78346633911133, Validation Loss: 50.64125061035156\n",
      "Epoch: 11/1000, Training Loss: 50.67353057861328, Validation Loss: 48.47507858276367\n",
      "Epoch: 12/1000, Training Loss: 48.49757385253906, Validation Loss: 46.88352584838867\n",
      "Epoch: 13/1000, Training Loss: 46.86756896972656, Validation Loss: 45.65426254272461\n",
      "Epoch: 14/1000, Training Loss: 45.63673400878906, Validation Loss: 44.32411193847656\n",
      "Epoch: 15/1000, Training Loss: 44.44953536987305, Validation Loss: 42.75132369995117\n",
      "Epoch: 16/1000, Training Loss: 42.62083435058594, Validation Loss: 41.6085319519043\n",
      "Epoch: 17/1000, Training Loss: 41.51997375488281, Validation Loss: 40.62054443359375\n",
      "Epoch: 18/1000, Training Loss: 40.80611801147461, Validation Loss: 40.46531295776367\n",
      "Epoch: 19/1000, Training Loss: 40.4832649230957, Validation Loss: 40.20463943481445\n",
      "Epoch: 20/1000, Training Loss: 40.39346694946289, Validation Loss: 40.05320358276367\n",
      "Epoch: 21/1000, Training Loss: 40.216251373291016, Validation Loss: 39.30497741699219\n",
      "Epoch: 22/1000, Training Loss: 39.62801742553711, Validation Loss: 38.75624084472656\n",
      "Epoch: 23/1000, Training Loss: 39.18196487426758, Validation Loss: 38.546085357666016\n",
      "Epoch: 24/1000, Training Loss: 39.04234313964844, Validation Loss: 38.16425704956055\n",
      "Epoch: 25/1000, Training Loss: 38.679866790771484, Validation Loss: 37.85691452026367\n",
      "Epoch: 26/1000, Training Loss: 38.24345397949219, Validation Loss: 37.3917121887207\n",
      "Epoch: 27/1000, Training Loss: 37.838314056396484, Validation Loss: 37.096923828125\n",
      "Epoch: 28/1000, Training Loss: 37.534000396728516, Validation Loss: 37.03133010864258\n",
      "Epoch: 29/1000, Training Loss: 37.46355438232422, Validation Loss: 37.00471115112305\n",
      "Epoch: 30/1000, Training Loss: 37.40750503540039, Validation Loss: 36.8404541015625\n",
      "Epoch: 31/1000, Training Loss: 37.305747985839844, Validation Loss: 36.773582458496094\n",
      "Epoch: 32/1000, Training Loss: 37.214744567871094, Validation Loss: 36.65721130371094\n",
      "Epoch: 33/1000, Training Loss: 37.133583068847656, Validation Loss: 36.52909469604492\n",
      "Epoch: 34/1000, Training Loss: 36.99089431762695, Validation Loss: 36.41244125366211\n",
      "Epoch: 35/1000, Training Loss: 36.93194580078125, Validation Loss: 36.245635986328125\n",
      "Epoch: 36/1000, Training Loss: 36.79128646850586, Validation Loss: 36.146324157714844\n",
      "Epoch: 37/1000, Training Loss: 36.683528900146484, Validation Loss: 36.04796600341797\n",
      "Epoch: 38/1000, Training Loss: 36.555686950683594, Validation Loss: 35.87760925292969\n",
      "Epoch: 39/1000, Training Loss: 36.43319320678711, Validation Loss: 35.74170684814453\n",
      "Epoch: 40/1000, Training Loss: 36.270591735839844, Validation Loss: 35.61708450317383\n",
      "Epoch: 41/1000, Training Loss: 36.13654708862305, Validation Loss: 35.49913024902344\n",
      "Epoch: 42/1000, Training Loss: 35.963260650634766, Validation Loss: 35.41642379760742\n",
      "Epoch: 43/1000, Training Loss: 35.77741622924805, Validation Loss: 35.281925201416016\n",
      "Epoch: 44/1000, Training Loss: 35.61817169189453, Validation Loss: 34.951416015625\n",
      "Epoch: 45/1000, Training Loss: 35.498497009277344, Validation Loss: 34.87510681152344\n",
      "Epoch: 46/1000, Training Loss: 35.355735778808594, Validation Loss: 34.5364990234375\n",
      "Epoch: 47/1000, Training Loss: 35.249305725097656, Validation Loss: 34.32994842529297\n",
      "Epoch: 48/1000, Training Loss: 35.09454345703125, Validation Loss: 34.16164016723633\n",
      "Epoch: 49/1000, Training Loss: 34.947120666503906, Validation Loss: 34.00543212890625\n",
      "Epoch: 50/1000, Training Loss: 34.7933349609375, Validation Loss: 33.8528938293457\n",
      "Epoch: 51/1000, Training Loss: 34.639312744140625, Validation Loss: 33.7960205078125\n",
      "Epoch: 52/1000, Training Loss: 34.47299575805664, Validation Loss: 33.67951965332031\n",
      "Epoch: 53/1000, Training Loss: 34.28956985473633, Validation Loss: 33.480918884277344\n",
      "Epoch: 54/1000, Training Loss: 34.065765380859375, Validation Loss: 33.320674896240234\n",
      "Epoch: 55/1000, Training Loss: 33.86061477661133, Validation Loss: 33.09286117553711\n",
      "Epoch: 56/1000, Training Loss: 33.69526290893555, Validation Loss: 33.01365661621094\n",
      "Epoch: 57/1000, Training Loss: 33.514286041259766, Validation Loss: 32.787864685058594\n",
      "Epoch: 58/1000, Training Loss: 33.39569091796875, Validation Loss: 32.49089813232422\n",
      "Epoch: 59/1000, Training Loss: 33.27488327026367, Validation Loss: 32.22994613647461\n",
      "Epoch: 60/1000, Training Loss: 33.128719329833984, Validation Loss: 31.934526443481445\n",
      "Epoch: 61/1000, Training Loss: 32.980594635009766, Validation Loss: 31.806156158447266\n",
      "Epoch: 62/1000, Training Loss: 32.81707000732422, Validation Loss: 31.647043228149414\n",
      "Epoch: 63/1000, Training Loss: 32.66493225097656, Validation Loss: 31.37776756286621\n",
      "Epoch: 64/1000, Training Loss: 32.49843978881836, Validation Loss: 31.303848266601562\n",
      "Epoch: 65/1000, Training Loss: 32.333892822265625, Validation Loss: 31.091493606567383\n",
      "Epoch: 66/1000, Training Loss: 32.123958587646484, Validation Loss: 30.82905387878418\n",
      "Epoch: 67/1000, Training Loss: 31.96224594116211, Validation Loss: 30.684307098388672\n",
      "Epoch: 68/1000, Training Loss: 31.85492706298828, Validation Loss: 30.605392456054688\n",
      "Epoch: 69/1000, Training Loss: 31.70467185974121, Validation Loss: 30.439592361450195\n",
      "Epoch: 70/1000, Training Loss: 31.529693603515625, Validation Loss: 30.456979751586914\n",
      "Epoch: 71/1000, Training Loss: 31.332956314086914, Validation Loss: 30.377201080322266\n",
      "Epoch: 72/1000, Training Loss: 31.16911506652832, Validation Loss: 30.23464012145996\n",
      "Epoch: 73/1000, Training Loss: 31.026844024658203, Validation Loss: 30.180570602416992\n",
      "Epoch: 74/1000, Training Loss: 30.899002075195312, Validation Loss: 30.100614547729492\n",
      "Epoch: 75/1000, Training Loss: 30.722156524658203, Validation Loss: 30.019487380981445\n",
      "Epoch: 76/1000, Training Loss: 30.511751174926758, Validation Loss: 29.8072566986084\n",
      "Epoch: 77/1000, Training Loss: 30.292503356933594, Validation Loss: 29.702041625976562\n",
      "Epoch: 78/1000, Training Loss: 30.116655349731445, Validation Loss: 29.649028778076172\n",
      "Epoch: 79/1000, Training Loss: 29.84640884399414, Validation Loss: 29.45153045654297\n",
      "Epoch: 80/1000, Training Loss: 29.712968826293945, Validation Loss: 29.358924865722656\n",
      "Epoch: 81/1000, Training Loss: 29.495615005493164, Validation Loss: 29.20878028869629\n",
      "Epoch: 82/1000, Training Loss: 29.330421447753906, Validation Loss: 29.078258514404297\n",
      "Epoch: 83/1000, Training Loss: 29.150117874145508, Validation Loss: 28.88889503479004\n",
      "Epoch: 84/1000, Training Loss: 28.980836868286133, Validation Loss: 28.79707145690918\n",
      "Epoch: 85/1000, Training Loss: 28.782794952392578, Validation Loss: 28.63554573059082\n",
      "Epoch: 86/1000, Training Loss: 28.67206382751465, Validation Loss: 28.39664649963379\n",
      "Epoch: 87/1000, Training Loss: 28.48406982421875, Validation Loss: 28.24144744873047\n",
      "Epoch: 88/1000, Training Loss: 28.294689178466797, Validation Loss: 28.051551818847656\n",
      "Epoch: 89/1000, Training Loss: 28.171649932861328, Validation Loss: 27.98887825012207\n",
      "Epoch: 90/1000, Training Loss: 28.018003463745117, Validation Loss: 27.8786563873291\n",
      "Epoch: 91/1000, Training Loss: 27.846715927124023, Validation Loss: 27.729204177856445\n",
      "Epoch: 92/1000, Training Loss: 27.7544002532959, Validation Loss: 27.59055519104004\n",
      "Epoch: 93/1000, Training Loss: 27.630477905273438, Validation Loss: 27.526803970336914\n",
      "Epoch: 94/1000, Training Loss: 27.486114501953125, Validation Loss: 27.3284969329834\n",
      "Epoch: 95/1000, Training Loss: 27.351806640625, Validation Loss: 27.178606033325195\n",
      "Epoch: 96/1000, Training Loss: 27.169124603271484, Validation Loss: 26.99134063720703\n",
      "Epoch: 97/1000, Training Loss: 27.032262802124023, Validation Loss: 26.883495330810547\n",
      "Epoch: 98/1000, Training Loss: 26.854772567749023, Validation Loss: 26.712326049804688\n",
      "Epoch: 99/1000, Training Loss: 26.67746353149414, Validation Loss: 26.410573959350586\n",
      "Epoch: 100/1000, Training Loss: 26.506595611572266, Validation Loss: 26.206581115722656\n",
      "Epoch: 101/1000, Training Loss: 26.35899543762207, Validation Loss: 26.057703018188477\n",
      "Epoch: 102/1000, Training Loss: 26.273941040039062, Validation Loss: 25.90338134765625\n",
      "Epoch: 103/1000, Training Loss: 26.17027473449707, Validation Loss: 25.835346221923828\n",
      "Epoch: 104/1000, Training Loss: 26.035707473754883, Validation Loss: 25.64140510559082\n",
      "Epoch: 105/1000, Training Loss: 25.91651725769043, Validation Loss: 25.45250129699707\n",
      "Epoch: 106/1000, Training Loss: 25.776086807250977, Validation Loss: 25.262739181518555\n",
      "Epoch: 107/1000, Training Loss: 25.686546325683594, Validation Loss: 25.219131469726562\n",
      "Epoch: 108/1000, Training Loss: 25.615760803222656, Validation Loss: 25.080429077148438\n",
      "Epoch: 109/1000, Training Loss: 25.492475509643555, Validation Loss: 24.96881675720215\n",
      "Epoch: 110/1000, Training Loss: 25.43136215209961, Validation Loss: 24.82451629638672\n",
      "Epoch: 111/1000, Training Loss: 25.332582473754883, Validation Loss: 24.741453170776367\n",
      "Epoch: 112/1000, Training Loss: 25.22749900817871, Validation Loss: 24.691377639770508\n",
      "Epoch: 113/1000, Training Loss: 25.174495697021484, Validation Loss: 24.621479034423828\n",
      "Epoch: 114/1000, Training Loss: 25.106794357299805, Validation Loss: 24.554746627807617\n",
      "Epoch: 115/1000, Training Loss: 25.02785301208496, Validation Loss: 24.485076904296875\n",
      "Epoch: 116/1000, Training Loss: 24.940702438354492, Validation Loss: 24.376848220825195\n",
      "Epoch: 117/1000, Training Loss: 24.851516723632812, Validation Loss: 24.290422439575195\n",
      "Epoch: 118/1000, Training Loss: 24.747053146362305, Validation Loss: 24.207744598388672\n",
      "Epoch: 119/1000, Training Loss: 24.652135848999023, Validation Loss: 24.174541473388672\n",
      "Epoch: 120/1000, Training Loss: 24.5616397857666, Validation Loss: 24.082420349121094\n",
      "Epoch: 121/1000, Training Loss: 24.465761184692383, Validation Loss: 23.98406219482422\n",
      "Epoch: 122/1000, Training Loss: 24.402807235717773, Validation Loss: 23.935686111450195\n",
      "Epoch: 123/1000, Training Loss: 24.312049865722656, Validation Loss: 23.85767364501953\n",
      "Epoch: 124/1000, Training Loss: 24.23054313659668, Validation Loss: 23.7825984954834\n",
      "Epoch: 125/1000, Training Loss: 24.17369842529297, Validation Loss: 23.74869155883789\n",
      "Epoch: 126/1000, Training Loss: 24.068439483642578, Validation Loss: 23.669294357299805\n",
      "Epoch: 127/1000, Training Loss: 23.97825813293457, Validation Loss: 23.579675674438477\n",
      "Epoch: 128/1000, Training Loss: 23.861879348754883, Validation Loss: 23.488967895507812\n",
      "Epoch: 129/1000, Training Loss: 23.7358455657959, Validation Loss: 23.406099319458008\n",
      "Epoch: 130/1000, Training Loss: 23.669340133666992, Validation Loss: 23.321828842163086\n",
      "Epoch: 131/1000, Training Loss: 23.621618270874023, Validation Loss: 23.187917709350586\n",
      "Epoch: 132/1000, Training Loss: 23.52509880065918, Validation Loss: 23.080442428588867\n",
      "Epoch: 133/1000, Training Loss: 23.456478118896484, Validation Loss: 23.09749984741211\n",
      "Epoch: 134/1000, Training Loss: 23.32273292541504, Validation Loss: 23.071325302124023\n",
      "Epoch: 135/1000, Training Loss: 23.24385643005371, Validation Loss: 23.01810073852539\n",
      "Epoch: 136/1000, Training Loss: 23.152284622192383, Validation Loss: 22.967660903930664\n",
      "Epoch: 137/1000, Training Loss: 23.054302215576172, Validation Loss: 22.870222091674805\n",
      "Epoch: 138/1000, Training Loss: 22.970726013183594, Validation Loss: 22.761396408081055\n",
      "Epoch: 139/1000, Training Loss: 22.867782592773438, Validation Loss: 22.6341495513916\n",
      "Epoch: 140/1000, Training Loss: 22.775104522705078, Validation Loss: 22.605690002441406\n",
      "Epoch: 141/1000, Training Loss: 22.647977828979492, Validation Loss: 22.524627685546875\n",
      "Epoch: 142/1000, Training Loss: 22.54922103881836, Validation Loss: 22.457334518432617\n",
      "Epoch: 143/1000, Training Loss: 22.436412811279297, Validation Loss: 22.331863403320312\n",
      "Epoch: 144/1000, Training Loss: 22.30451011657715, Validation Loss: 22.293100357055664\n",
      "Epoch: 145/1000, Training Loss: 22.1540470123291, Validation Loss: 22.194028854370117\n",
      "Epoch: 146/1000, Training Loss: 21.960939407348633, Validation Loss: 21.992998123168945\n",
      "Epoch: 147/1000, Training Loss: 21.799936294555664, Validation Loss: 21.841054916381836\n",
      "Epoch: 148/1000, Training Loss: 21.660507202148438, Validation Loss: 21.672027587890625\n",
      "Epoch: 149/1000, Training Loss: 21.534399032592773, Validation Loss: 21.51697540283203\n",
      "Epoch: 150/1000, Training Loss: 21.382368087768555, Validation Loss: 21.407196044921875\n",
      "Epoch: 151/1000, Training Loss: 21.3116455078125, Validation Loss: 21.31825828552246\n",
      "Epoch: 152/1000, Training Loss: 21.20709800720215, Validation Loss: 21.241666793823242\n",
      "Epoch: 153/1000, Training Loss: 21.076086044311523, Validation Loss: 21.122982025146484\n",
      "Epoch: 154/1000, Training Loss: 20.895753860473633, Validation Loss: 20.93661880493164\n",
      "Epoch: 155/1000, Training Loss: 20.700443267822266, Validation Loss: 20.822416305541992\n",
      "Epoch: 156/1000, Training Loss: 20.54963493347168, Validation Loss: 20.668031692504883\n",
      "Epoch: 157/1000, Training Loss: 20.41553497314453, Validation Loss: 20.517906188964844\n",
      "Epoch: 158/1000, Training Loss: 20.212038040161133, Validation Loss: 20.40736198425293\n",
      "Epoch: 159/1000, Training Loss: 20.08324432373047, Validation Loss: 20.159902572631836\n",
      "Epoch: 160/1000, Training Loss: 19.9208927154541, Validation Loss: 19.980886459350586\n",
      "Epoch: 161/1000, Training Loss: 19.83474349975586, Validation Loss: 19.835979461669922\n",
      "Epoch: 162/1000, Training Loss: 19.71851921081543, Validation Loss: 19.696836471557617\n",
      "Epoch: 163/1000, Training Loss: 19.570283889770508, Validation Loss: 19.53954315185547\n",
      "Epoch: 164/1000, Training Loss: 19.458709716796875, Validation Loss: 19.423126220703125\n",
      "Epoch: 165/1000, Training Loss: 19.326440811157227, Validation Loss: 19.305782318115234\n",
      "Epoch: 166/1000, Training Loss: 19.2003116607666, Validation Loss: 19.145795822143555\n",
      "Epoch: 167/1000, Training Loss: 19.040754318237305, Validation Loss: 19.110977172851562\n",
      "Epoch: 168/1000, Training Loss: 18.920785903930664, Validation Loss: 18.93014144897461\n",
      "Epoch: 169/1000, Training Loss: 18.77552032470703, Validation Loss: 18.615352630615234\n",
      "Epoch: 170/1000, Training Loss: 18.66366195678711, Validation Loss: 18.476531982421875\n",
      "Epoch: 171/1000, Training Loss: 18.51902961730957, Validation Loss: 18.432464599609375\n",
      "Epoch: 172/1000, Training Loss: 18.397890090942383, Validation Loss: 18.354843139648438\n",
      "Epoch: 173/1000, Training Loss: 18.27088165283203, Validation Loss: 18.205480575561523\n",
      "Epoch: 174/1000, Training Loss: 18.118009567260742, Validation Loss: 17.87460708618164\n",
      "Epoch: 175/1000, Training Loss: 17.874284744262695, Validation Loss: 17.735187530517578\n",
      "Epoch: 176/1000, Training Loss: 17.615808486938477, Validation Loss: 17.598251342773438\n",
      "Epoch: 177/1000, Training Loss: 17.335575103759766, Validation Loss: 17.25518035888672\n",
      "Epoch: 178/1000, Training Loss: 17.172130584716797, Validation Loss: 16.947158813476562\n",
      "Epoch: 179/1000, Training Loss: 16.965717315673828, Validation Loss: 16.762447357177734\n",
      "Epoch: 180/1000, Training Loss: 16.766820907592773, Validation Loss: 16.602325439453125\n",
      "Epoch: 181/1000, Training Loss: 16.6341495513916, Validation Loss: 16.358179092407227\n",
      "Epoch: 182/1000, Training Loss: 16.474037170410156, Validation Loss: 16.2404842376709\n",
      "Epoch: 183/1000, Training Loss: 16.345779418945312, Validation Loss: 16.146215438842773\n",
      "Epoch: 184/1000, Training Loss: 16.18647575378418, Validation Loss: 15.919393539428711\n",
      "Epoch: 185/1000, Training Loss: 16.065887451171875, Validation Loss: 15.634390830993652\n",
      "Epoch: 186/1000, Training Loss: 15.898083686828613, Validation Loss: 15.529471397399902\n",
      "Epoch: 187/1000, Training Loss: 15.739679336547852, Validation Loss: 15.426003456115723\n",
      "Epoch: 188/1000, Training Loss: 15.56804084777832, Validation Loss: 15.319215774536133\n",
      "Epoch: 189/1000, Training Loss: 15.470685958862305, Validation Loss: 15.186376571655273\n",
      "Epoch: 190/1000, Training Loss: 15.306852340698242, Validation Loss: 15.01512622833252\n",
      "Epoch: 191/1000, Training Loss: 15.133430480957031, Validation Loss: 14.94007682800293\n",
      "Epoch: 192/1000, Training Loss: 14.943719863891602, Validation Loss: 14.759834289550781\n",
      "Epoch: 193/1000, Training Loss: 14.786528587341309, Validation Loss: 14.606050491333008\n",
      "Epoch: 194/1000, Training Loss: 14.604866981506348, Validation Loss: 14.409916877746582\n",
      "Epoch: 195/1000, Training Loss: 14.44800090789795, Validation Loss: 14.208342552185059\n",
      "Epoch: 196/1000, Training Loss: 14.263985633850098, Validation Loss: 14.011279106140137\n",
      "Epoch: 197/1000, Training Loss: 14.105528831481934, Validation Loss: 13.82741641998291\n",
      "Epoch: 198/1000, Training Loss: 13.95771598815918, Validation Loss: 13.590055465698242\n",
      "Epoch: 199/1000, Training Loss: 13.828965187072754, Validation Loss: 13.553200721740723\n",
      "Epoch: 200/1000, Training Loss: 13.729717254638672, Validation Loss: 13.506791114807129\n",
      "Epoch: 201/1000, Training Loss: 13.616695404052734, Validation Loss: 13.475015640258789\n",
      "Epoch: 202/1000, Training Loss: 13.540655136108398, Validation Loss: 13.428607940673828\n",
      "Epoch: 203/1000, Training Loss: 13.471553802490234, Validation Loss: 13.417941093444824\n",
      "Epoch: 204/1000, Training Loss: 13.404151916503906, Validation Loss: 13.23338508605957\n",
      "Epoch: 205/1000, Training Loss: 13.3455810546875, Validation Loss: 13.126639366149902\n",
      "Epoch: 206/1000, Training Loss: 13.238199234008789, Validation Loss: 13.044325828552246\n",
      "Epoch: 207/1000, Training Loss: 13.15794563293457, Validation Loss: 13.067628860473633\n",
      "Epoch: 208/1000, Training Loss: 13.048624038696289, Validation Loss: 12.967852592468262\n",
      "Epoch: 209/1000, Training Loss: 12.982426643371582, Validation Loss: 12.886068344116211\n",
      "Epoch: 210/1000, Training Loss: 12.902549743652344, Validation Loss: 12.828248023986816\n",
      "Epoch: 211/1000, Training Loss: 12.817358016967773, Validation Loss: 12.698150634765625\n",
      "Epoch: 212/1000, Training Loss: 12.774658203125, Validation Loss: 12.588793754577637\n",
      "Epoch: 213/1000, Training Loss: 12.694466590881348, Validation Loss: 12.506965637207031\n",
      "Epoch: 214/1000, Training Loss: 12.653526306152344, Validation Loss: 12.50195598602295\n",
      "Epoch: 215/1000, Training Loss: 12.586034774780273, Validation Loss: 12.426163673400879\n",
      "Epoch: 216/1000, Training Loss: 12.505365371704102, Validation Loss: 12.375104904174805\n",
      "Epoch: 217/1000, Training Loss: 12.423812866210938, Validation Loss: 12.311893463134766\n",
      "Epoch: 218/1000, Training Loss: 12.369400978088379, Validation Loss: 12.216166496276855\n",
      "Epoch: 219/1000, Training Loss: 12.299966812133789, Validation Loss: 12.17042064666748\n",
      "Epoch: 220/1000, Training Loss: 12.217819213867188, Validation Loss: 12.128024101257324\n",
      "Epoch: 221/1000, Training Loss: 12.160369873046875, Validation Loss: 12.071538925170898\n",
      "Epoch: 222/1000, Training Loss: 12.100990295410156, Validation Loss: 11.98711109161377\n",
      "Epoch: 223/1000, Training Loss: 12.044022560119629, Validation Loss: 11.982122421264648\n",
      "Epoch: 224/1000, Training Loss: 11.977471351623535, Validation Loss: 11.968557357788086\n",
      "Epoch: 225/1000, Training Loss: 11.953949928283691, Validation Loss: 11.907805442810059\n",
      "Epoch: 226/1000, Training Loss: 11.861092567443848, Validation Loss: 11.823090553283691\n",
      "Epoch: 227/1000, Training Loss: 11.770517349243164, Validation Loss: 11.715868949890137\n",
      "Epoch: 228/1000, Training Loss: 11.731796264648438, Validation Loss: 11.683847427368164\n",
      "Epoch: 229/1000, Training Loss: 11.629966735839844, Validation Loss: 11.54344367980957\n",
      "Epoch: 230/1000, Training Loss: 11.537429809570312, Validation Loss: 11.425043106079102\n",
      "Epoch: 231/1000, Training Loss: 11.450065612792969, Validation Loss: 11.29531478881836\n",
      "Epoch: 232/1000, Training Loss: 11.324183464050293, Validation Loss: 11.003381729125977\n",
      "Epoch: 233/1000, Training Loss: 11.208030700683594, Validation Loss: 10.916683197021484\n",
      "Epoch: 234/1000, Training Loss: 11.129647254943848, Validation Loss: 10.75680160522461\n",
      "Epoch: 235/1000, Training Loss: 11.041947364807129, Validation Loss: 10.60036849975586\n",
      "Epoch: 236/1000, Training Loss: 10.976847648620605, Validation Loss: 10.490622520446777\n",
      "Epoch: 237/1000, Training Loss: 10.884947776794434, Validation Loss: 10.414445877075195\n",
      "Epoch: 238/1000, Training Loss: 10.788865089416504, Validation Loss: 10.361766815185547\n",
      "Epoch: 239/1000, Training Loss: 10.704105377197266, Validation Loss: 10.366546630859375\n",
      "Epoch: 240/1000, Training Loss: 10.645896911621094, Validation Loss: 10.172405242919922\n",
      "Epoch: 241/1000, Training Loss: 10.584554672241211, Validation Loss: 10.162662506103516\n",
      "Epoch: 242/1000, Training Loss: 10.528944969177246, Validation Loss: 10.098374366760254\n",
      "Epoch: 243/1000, Training Loss: 10.439857482910156, Validation Loss: 10.044756889343262\n",
      "Epoch: 244/1000, Training Loss: 10.37375545501709, Validation Loss: 10.002185821533203\n",
      "Epoch: 245/1000, Training Loss: 10.25778579711914, Validation Loss: 9.984439849853516\n",
      "Epoch: 246/1000, Training Loss: 10.15776252746582, Validation Loss: 9.898438453674316\n",
      "Epoch: 247/1000, Training Loss: 10.095198631286621, Validation Loss: 9.858733177185059\n",
      "Epoch: 248/1000, Training Loss: 10.038456916809082, Validation Loss: 9.859870910644531\n",
      "Epoch: 249/1000, Training Loss: 9.965863227844238, Validation Loss: 9.835247993469238\n",
      "Epoch: 250/1000, Training Loss: 9.938793182373047, Validation Loss: 9.781791687011719\n",
      "Epoch: 251/1000, Training Loss: 9.89880084991455, Validation Loss: 9.739876747131348\n",
      "Epoch: 252/1000, Training Loss: 9.860021591186523, Validation Loss: 9.724223136901855\n",
      "Epoch: 253/1000, Training Loss: 9.79172134399414, Validation Loss: 9.733683586120605\n",
      "Epoch: 254/1000, Training Loss: 9.742856979370117, Validation Loss: 9.682228088378906\n",
      "Epoch: 255/1000, Training Loss: 9.686972618103027, Validation Loss: 9.653173446655273\n",
      "Epoch: 256/1000, Training Loss: 9.60157585144043, Validation Loss: 9.576032638549805\n",
      "Epoch: 257/1000, Training Loss: 9.53247356414795, Validation Loss: 9.463969230651855\n",
      "Epoch: 258/1000, Training Loss: 9.48755168914795, Validation Loss: 9.411023139953613\n",
      "Epoch: 259/1000, Training Loss: 9.434342384338379, Validation Loss: 9.326851844787598\n",
      "Epoch: 260/1000, Training Loss: 9.412287712097168, Validation Loss: 9.328165054321289\n",
      "Epoch: 261/1000, Training Loss: 9.345718383789062, Validation Loss: 9.248292922973633\n",
      "Epoch: 262/1000, Training Loss: 9.290224075317383, Validation Loss: 9.221517562866211\n",
      "Epoch: 263/1000, Training Loss: 9.253226280212402, Validation Loss: 9.143106460571289\n",
      "Epoch: 264/1000, Training Loss: 9.236176490783691, Validation Loss: 9.14059066772461\n",
      "Epoch: 265/1000, Training Loss: 9.212104797363281, Validation Loss: 9.139991760253906\n",
      "Epoch: 266/1000, Training Loss: 9.170201301574707, Validation Loss: 9.140368461608887\n",
      "Epoch: 267/1000, Training Loss: 9.161910057067871, Validation Loss: 9.126984596252441\n",
      "Epoch: 268/1000, Training Loss: 9.114625930786133, Validation Loss: 9.023783683776855\n",
      "Epoch: 269/1000, Training Loss: 9.095296859741211, Validation Loss: 9.00312614440918\n",
      "Epoch: 270/1000, Training Loss: 9.052974700927734, Validation Loss: 8.990711212158203\n",
      "Epoch: 271/1000, Training Loss: 9.021042823791504, Validation Loss: 8.95826244354248\n",
      "Epoch: 272/1000, Training Loss: 8.974406242370605, Validation Loss: 8.928787231445312\n",
      "Epoch: 273/1000, Training Loss: 8.93362045288086, Validation Loss: 8.849528312683105\n",
      "Epoch: 274/1000, Training Loss: 8.890256881713867, Validation Loss: 8.783945083618164\n",
      "Epoch: 275/1000, Training Loss: 8.837244987487793, Validation Loss: 8.727507591247559\n",
      "Epoch: 276/1000, Training Loss: 8.788341522216797, Validation Loss: 8.719880104064941\n",
      "Epoch: 277/1000, Training Loss: 8.73912525177002, Validation Loss: 8.644375801086426\n",
      "Epoch: 278/1000, Training Loss: 8.711410522460938, Validation Loss: 8.614678382873535\n",
      "Epoch: 279/1000, Training Loss: 8.687895774841309, Validation Loss: 8.58292007446289\n",
      "Epoch: 280/1000, Training Loss: 8.681679725646973, Validation Loss: 8.571147918701172\n",
      "Epoch: 281/1000, Training Loss: 8.660592079162598, Validation Loss: 8.479653358459473\n",
      "Epoch: 282/1000, Training Loss: 8.63737964630127, Validation Loss: 8.422159194946289\n",
      "Epoch: 283/1000, Training Loss: 8.60049057006836, Validation Loss: 8.318370819091797\n",
      "Epoch: 284/1000, Training Loss: 8.570561408996582, Validation Loss: 8.284768104553223\n",
      "Epoch: 285/1000, Training Loss: 8.545427322387695, Validation Loss: 8.205078125\n",
      "Epoch: 286/1000, Training Loss: 8.51356315612793, Validation Loss: 8.182049751281738\n",
      "Epoch: 287/1000, Training Loss: 8.490328788757324, Validation Loss: 8.155494689941406\n",
      "Epoch: 288/1000, Training Loss: 8.468963623046875, Validation Loss: 8.119555473327637\n",
      "Epoch: 289/1000, Training Loss: 8.43836498260498, Validation Loss: 8.113943099975586\n",
      "Epoch: 290/1000, Training Loss: 8.411952018737793, Validation Loss: 8.106485366821289\n",
      "Epoch: 291/1000, Training Loss: 8.376910209655762, Validation Loss: 8.075135231018066\n",
      "Epoch: 292/1000, Training Loss: 8.350895881652832, Validation Loss: 8.085134506225586\n",
      "Epoch: 293/1000, Training Loss: 8.335468292236328, Validation Loss: 8.05102825164795\n",
      "Epoch: 294/1000, Training Loss: 8.304189682006836, Validation Loss: 8.047598838806152\n",
      "Epoch: 295/1000, Training Loss: 8.281173706054688, Validation Loss: 7.949960231781006\n",
      "Epoch: 296/1000, Training Loss: 8.234495162963867, Validation Loss: 7.911801815032959\n",
      "Epoch: 297/1000, Training Loss: 8.18791389465332, Validation Loss: 7.902904033660889\n",
      "Epoch: 298/1000, Training Loss: 8.132896423339844, Validation Loss: 7.823696613311768\n",
      "Epoch: 299/1000, Training Loss: 8.106219291687012, Validation Loss: 7.731916427612305\n",
      "Epoch: 300/1000, Training Loss: 8.059906959533691, Validation Loss: 7.677798748016357\n",
      "Epoch: 301/1000, Training Loss: 8.0396146774292, Validation Loss: 7.619747638702393\n",
      "Epoch: 302/1000, Training Loss: 8.018689155578613, Validation Loss: 7.617557525634766\n",
      "Epoch: 303/1000, Training Loss: 7.996281147003174, Validation Loss: 7.560756206512451\n",
      "Epoch: 304/1000, Training Loss: 7.966867923736572, Validation Loss: 7.508816242218018\n",
      "Epoch: 305/1000, Training Loss: 7.942785263061523, Validation Loss: 7.495655536651611\n",
      "Epoch: 306/1000, Training Loss: 7.881015777587891, Validation Loss: 7.488698959350586\n",
      "Epoch: 307/1000, Training Loss: 7.858415126800537, Validation Loss: 7.465573787689209\n",
      "Epoch: 308/1000, Training Loss: 7.803061485290527, Validation Loss: 7.42283821105957\n",
      "Epoch: 309/1000, Training Loss: 7.788693904876709, Validation Loss: 7.420301914215088\n",
      "Epoch: 310/1000, Training Loss: 7.770917892456055, Validation Loss: 7.393728256225586\n",
      "Epoch: 311/1000, Training Loss: 7.740954875946045, Validation Loss: 7.366588115692139\n",
      "Epoch: 312/1000, Training Loss: 7.710031032562256, Validation Loss: 7.364801406860352\n",
      "Epoch: 313/1000, Training Loss: 7.662651062011719, Validation Loss: 7.337184429168701\n",
      "Epoch: 314/1000, Training Loss: 7.6456098556518555, Validation Loss: 7.335676193237305\n",
      "Epoch: 315/1000, Training Loss: 7.622467994689941, Validation Loss: 7.3349738121032715\n",
      "Epoch: 316/1000, Training Loss: 7.584467887878418, Validation Loss: 7.309956073760986\n",
      "Epoch: 317/1000, Training Loss: 7.568360805511475, Validation Loss: 7.306344985961914\n",
      "Epoch: 318/1000, Training Loss: 7.545074939727783, Validation Loss: 7.306647300720215\n",
      "Epoch: 319/1000, Training Loss: 7.5368218421936035, Validation Loss: 7.281792640686035\n",
      "Epoch: 320/1000, Training Loss: 7.51460599899292, Validation Loss: 7.251476287841797\n",
      "Epoch: 321/1000, Training Loss: 7.484969615936279, Validation Loss: 7.252791404724121\n",
      "Epoch: 322/1000, Training Loss: 7.474363803863525, Validation Loss: 7.254054069519043\n",
      "Epoch: 323/1000, Training Loss: 7.473288059234619, Validation Loss: 7.227048397064209\n",
      "Epoch: 324/1000, Training Loss: 7.446434497833252, Validation Loss: 7.227110385894775\n",
      "Epoch: 325/1000, Training Loss: 7.421900272369385, Validation Loss: 7.227346897125244\n",
      "Epoch: 326/1000, Training Loss: 7.4096760749816895, Validation Loss: 7.227819919586182\n",
      "Epoch: 327/1000, Training Loss: 7.387635231018066, Validation Loss: 7.22838830947876\n",
      "Epoch: 328/1000, Training Loss: 7.371315956115723, Validation Loss: 7.233761787414551\n",
      "Epoch: 329/1000, Training Loss: 7.362508296966553, Validation Loss: 7.185152530670166\n",
      "Epoch: 330/1000, Training Loss: 7.344676494598389, Validation Loss: 7.167074680328369\n",
      "Epoch: 331/1000, Training Loss: 7.344524383544922, Validation Loss: 7.170103549957275\n",
      "Epoch: 332/1000, Training Loss: 7.333996772766113, Validation Loss: 7.17390251159668\n",
      "Epoch: 333/1000, Training Loss: 7.315225124359131, Validation Loss: 7.174253940582275\n",
      "Epoch: 334/1000, Training Loss: 7.309600353240967, Validation Loss: 7.174717426300049\n",
      "Epoch: 335/1000, Training Loss: 7.309169769287109, Validation Loss: 7.175088882446289\n",
      "Epoch: 336/1000, Training Loss: 7.297895431518555, Validation Loss: 7.174829959869385\n",
      "Epoch: 337/1000, Training Loss: 7.278966903686523, Validation Loss: 7.1741180419921875\n",
      "Epoch: 338/1000, Training Loss: 7.273075580596924, Validation Loss: 7.173712253570557\n",
      "Epoch: 339/1000, Training Loss: 7.266005992889404, Validation Loss: 7.173807144165039\n",
      "Epoch: 340/1000, Training Loss: 7.248971939086914, Validation Loss: 7.182556629180908\n",
      "Epoch: 341/1000, Training Loss: 7.241855621337891, Validation Loss: 7.195181846618652\n",
      "Epoch: 342/1000, Training Loss: 7.233250141143799, Validation Loss: 7.204401969909668\n",
      "Epoch: 343/1000, Training Loss: 7.223862648010254, Validation Loss: 7.205181121826172\n",
      "Epoch: 344/1000, Training Loss: 7.219381809234619, Validation Loss: 7.209133625030518\n",
      "Epoch: 345/1000, Training Loss: 7.216839790344238, Validation Loss: 7.228332996368408\n",
      "Epoch: 346/1000, Training Loss: 7.209152698516846, Validation Loss: 7.209583759307861\n",
      "Epoch: 347/1000, Training Loss: 7.210122108459473, Validation Loss: 7.208196640014648\n",
      "Epoch: 348/1000, Training Loss: 7.193410396575928, Validation Loss: 7.1822919845581055\n",
      "Epoch: 349/1000, Training Loss: 7.194108486175537, Validation Loss: 7.18223762512207\n",
      "Epoch: 350/1000, Training Loss: 7.189403533935547, Validation Loss: 7.183265686035156\n",
      "Epoch: 351/1000, Training Loss: 7.18881893157959, Validation Loss: 7.184229373931885\n",
      "Epoch: 352/1000, Training Loss: 7.188456058502197, Validation Loss: 7.185142517089844\n",
      "Epoch: 353/1000, Training Loss: 7.182984352111816, Validation Loss: 7.186186790466309\n",
      "Epoch: 354/1000, Training Loss: 7.183658123016357, Validation Loss: 7.186466693878174\n",
      "Epoch: 355/1000, Training Loss: 7.184876918792725, Validation Loss: 7.185369968414307\n",
      "Epoch: 356/1000, Training Loss: 7.183643341064453, Validation Loss: 7.184370040893555\n",
      "Epoch: 357/1000, Training Loss: 7.182985782623291, Validation Loss: 7.1834588050842285\n",
      "Epoch: 358/1000, Training Loss: 7.18238639831543, Validation Loss: 7.182628631591797\n",
      "Epoch: 359/1000, Training Loss: 7.18183708190918, Validation Loss: 7.18187141418457\n",
      "Epoch: 360/1000, Training Loss: 7.181336402893066, Validation Loss: 7.18118143081665\n",
      "Epoch: 361/1000, Training Loss: 7.180877208709717, Validation Loss: 7.180551528930664\n",
      "Epoch: 362/1000, Training Loss: 7.180456161499023, Validation Loss: 7.179983139038086\n",
      "Epoch: 363/1000, Training Loss: 7.180070400238037, Validation Loss: 7.179481506347656\n",
      "Epoch: 364/1000, Training Loss: 7.178570747375488, Validation Loss: 7.179037570953369\n",
      "Epoch: 365/1000, Training Loss: 7.177469253540039, Validation Loss: 7.178515434265137\n",
      "Epoch: 366/1000, Training Loss: 7.175014972686768, Validation Loss: 7.178093433380127\n",
      "Epoch: 367/1000, Training Loss: 7.170767307281494, Validation Loss: 7.176889896392822\n",
      "Epoch: 368/1000, Training Loss: 7.167294979095459, Validation Loss: 7.1758012771606445\n",
      "Epoch: 369/1000, Training Loss: 7.1657185554504395, Validation Loss: 7.1748247146606445\n",
      "Epoch: 370/1000, Training Loss: 7.164348125457764, Validation Loss: 7.17527961730957\n",
      "Epoch: 371/1000, Training Loss: 7.163550853729248, Validation Loss: 7.177391052246094\n",
      "Epoch: 372/1000, Training Loss: 7.161031723022461, Validation Loss: 7.195044994354248\n",
      "Epoch: 373/1000, Training Loss: 7.157814979553223, Validation Loss: 7.19012451171875\n",
      "Epoch: 374/1000, Training Loss: 7.155818462371826, Validation Loss: 7.186051368713379\n",
      "Epoch: 375/1000, Training Loss: 7.1521430015563965, Validation Loss: 7.183215141296387\n",
      "Epoch: 376/1000, Training Loss: 7.148451805114746, Validation Loss: 7.179369926452637\n",
      "Epoch: 377/1000, Training Loss: 7.142610549926758, Validation Loss: 7.198780536651611\n",
      "Epoch: 378/1000, Training Loss: 7.13756799697876, Validation Loss: 7.189648151397705\n",
      "Epoch: 379/1000, Training Loss: 7.133765697479248, Validation Loss: 7.159548282623291\n",
      "Epoch: 380/1000, Training Loss: 7.1323089599609375, Validation Loss: 7.146042823791504\n",
      "Epoch: 381/1000, Training Loss: 7.129580974578857, Validation Loss: 7.142728805541992\n",
      "Epoch: 382/1000, Training Loss: 7.127987384796143, Validation Loss: 7.136809825897217\n",
      "Epoch: 383/1000, Training Loss: 7.127272129058838, Validation Loss: 7.107571601867676\n",
      "Epoch: 384/1000, Training Loss: 7.12662410736084, Validation Loss: 7.1031341552734375\n",
      "Epoch: 385/1000, Training Loss: 7.126036643981934, Validation Loss: 7.099957466125488\n",
      "Epoch: 386/1000, Training Loss: 7.125503063201904, Validation Loss: 7.098155498504639\n",
      "Epoch: 387/1000, Training Loss: 7.125020980834961, Validation Loss: 7.096625328063965\n",
      "Epoch: 388/1000, Training Loss: 7.124583721160889, Validation Loss: 7.095239162445068\n",
      "Epoch: 389/1000, Training Loss: 7.124187469482422, Validation Loss: 7.093770980834961\n",
      "Epoch: 390/1000, Training Loss: 7.1237382888793945, Validation Loss: 7.088292598724365\n",
      "Epoch: 391/1000, Training Loss: 7.121581554412842, Validation Loss: 7.07996129989624\n",
      "Epoch: 392/1000, Training Loss: 7.119571208953857, Validation Loss: 7.076935768127441\n",
      "Epoch: 393/1000, Training Loss: 7.117166996002197, Validation Loss: 7.0760416984558105\n",
      "Epoch: 394/1000, Training Loss: 7.113656997680664, Validation Loss: 7.075294017791748\n",
      "Epoch: 395/1000, Training Loss: 7.110359191894531, Validation Loss: 7.075588226318359\n",
      "Epoch: 396/1000, Training Loss: 7.1060261726379395, Validation Loss: 7.103749752044678\n",
      "Epoch: 397/1000, Training Loss: 7.103117942810059, Validation Loss: 7.103085994720459\n",
      "Epoch: 398/1000, Training Loss: 7.101439952850342, Validation Loss: 7.102616786956787\n",
      "Epoch: 399/1000, Training Loss: 7.100502967834473, Validation Loss: 7.10219669342041\n",
      "Epoch: 400/1000, Training Loss: 7.100294589996338, Validation Loss: 7.101804256439209\n",
      "Epoch: 401/1000, Training Loss: 7.1006760597229, Validation Loss: 7.101452350616455\n",
      "Epoch: 402/1000, Training Loss: 7.1003570556640625, Validation Loss: 7.101133346557617\n",
      "Epoch: 403/1000, Training Loss: 7.100069999694824, Validation Loss: 7.100844383239746\n",
      "Epoch: 404/1000, Training Loss: 7.099845886230469, Validation Loss: 7.100707530975342\n",
      "Epoch: 405/1000, Training Loss: 7.099703311920166, Validation Loss: 7.100844860076904\n",
      "Epoch: 406/1000, Training Loss: 7.099212169647217, Validation Loss: 7.124522686004639\n",
      "Epoch: 407/1000, Training Loss: 7.098917007446289, Validation Loss: 7.12438440322876\n",
      "Epoch: 408/1000, Training Loss: 7.098743915557861, Validation Loss: 7.124258518218994\n",
      "Epoch: 409/1000, Training Loss: 7.098627090454102, Validation Loss: 7.124147891998291\n",
      "Epoch: 410/1000, Training Loss: 7.0985236167907715, Validation Loss: 7.124048233032227\n",
      "Epoch: 411/1000, Training Loss: 7.09843111038208, Validation Loss: 7.123959541320801\n",
      "Epoch: 412/1000, Training Loss: 7.098347187042236, Validation Loss: 7.123881816864014\n",
      "Epoch: 413/1000, Training Loss: 7.098272800445557, Validation Loss: 7.123814105987549\n",
      "Epoch: 414/1000, Training Loss: 7.09820556640625, Validation Loss: 7.123754501342773\n",
      "Epoch: 415/1000, Training Loss: 7.098145961761475, Validation Loss: 7.123702526092529\n",
      "Epoch: 416/1000, Training Loss: 7.098092079162598, Validation Loss: 7.123654842376709\n",
      "Epoch: 417/1000, Training Loss: 7.0980448722839355, Validation Loss: 7.123611927032471\n",
      "Epoch: 418/1000, Training Loss: 7.0980024337768555, Validation Loss: 7.123574733734131\n",
      "Epoch: 419/1000, Training Loss: 7.097965240478516, Validation Loss: 7.123538970947266\n",
      "Epoch: 420/1000, Training Loss: 7.097930431365967, Validation Loss: 7.12350606918335\n",
      "Epoch: 421/1000, Training Loss: 7.097899436950684, Validation Loss: 7.123476505279541\n",
      "Epoch: 422/1000, Training Loss: 7.097869873046875, Validation Loss: 7.123448371887207\n",
      "Epoch: 423/1000, Training Loss: 7.097843647003174, Validation Loss: 7.123423099517822\n",
      "Epoch: 424/1000, Training Loss: 7.097818374633789, Validation Loss: 7.123398780822754\n",
      "Epoch: 425/1000, Training Loss: 7.0977959632873535, Validation Loss: 7.123376369476318\n",
      "Epoch: 426/1000, Training Loss: 7.097773551940918, Validation Loss: 7.123355388641357\n",
      "Epoch: 427/1000, Training Loss: 7.097753047943115, Validation Loss: 7.123335361480713\n",
      "Epoch: 428/1000, Training Loss: 7.097733974456787, Validation Loss: 7.123316287994385\n",
      "Epoch: 429/1000, Training Loss: 7.097716331481934, Validation Loss: 7.123298645019531\n",
      "Epoch: 430/1000, Training Loss: 7.097699165344238, Validation Loss: 7.123281478881836\n",
      "Epoch: 431/1000, Training Loss: 7.097681999206543, Validation Loss: 7.123266220092773\n",
      "Epoch: 432/1000, Training Loss: 7.0976667404174805, Validation Loss: 7.123249530792236\n",
      "Epoch: 433/1000, Training Loss: 7.097651958465576, Validation Loss: 7.12323522567749\n",
      "Epoch: 434/1000, Training Loss: 7.09763765335083, Validation Loss: 7.123220443725586\n",
      "Epoch: 435/1000, Training Loss: 7.097624778747559, Validation Loss: 7.123206615447998\n",
      "Epoch: 436/1000, Training Loss: 7.097611427307129, Validation Loss: 7.123193264007568\n",
      "Epoch: 437/1000, Training Loss: 7.097598075866699, Validation Loss: 7.123180866241455\n",
      "Epoch: 438/1000, Training Loss: 7.097586154937744, Validation Loss: 7.123167991638184\n",
      "Epoch: 439/1000, Training Loss: 7.097574710845947, Validation Loss: 7.123157024383545\n",
      "Epoch: 440/1000, Training Loss: 7.09756326675415, Validation Loss: 7.12314510345459\n",
      "Epoch: 441/1000, Training Loss: 7.097551345825195, Validation Loss: 7.123133182525635\n",
      "Epoch: 442/1000, Training Loss: 7.097539901733398, Validation Loss: 7.12312126159668\n",
      "Epoch: 443/1000, Training Loss: 7.097529888153076, Validation Loss: 7.123111248016357\n",
      "Epoch: 444/1000, Training Loss: 7.097518444061279, Validation Loss: 7.1230998039245605\n",
      "Epoch: 445/1000, Training Loss: 7.097508430480957, Validation Loss: 7.123088836669922\n",
      "Epoch: 446/1000, Training Loss: 7.097497940063477, Validation Loss: 7.1230788230896\n",
      "Epoch: 447/1000, Training Loss: 7.0974884033203125, Validation Loss: 7.123067855834961\n",
      "Epoch: 448/1000, Training Loss: 7.09747838973999, Validation Loss: 7.123058319091797\n",
      "Epoch: 449/1000, Training Loss: 7.097468376159668, Validation Loss: 7.123047351837158\n",
      "Epoch: 450/1000, Training Loss: 7.097458839416504, Validation Loss: 7.123038291931152\n",
      "Epoch: 451/1000, Training Loss: 7.09744930267334, Validation Loss: 7.123027801513672\n",
      "Epoch: 452/1000, Training Loss: 7.097440242767334, Validation Loss: 7.123018741607666\n",
      "Epoch: 453/1000, Training Loss: 7.097431659698486, Validation Loss: 7.123008728027344\n",
      "Epoch: 454/1000, Training Loss: 7.097421646118164, Validation Loss: 7.122999668121338\n",
      "Epoch: 455/1000, Training Loss: 7.097412586212158, Validation Loss: 7.122990608215332\n",
      "Epoch: 456/1000, Training Loss: 7.0974040031433105, Validation Loss: 7.12298059463501\n",
      "Epoch: 457/1000, Training Loss: 7.0973944664001465, Validation Loss: 7.122971534729004\n",
      "Epoch: 458/1000, Training Loss: 7.097385883331299, Validation Loss: 7.12296199798584\n",
      "Epoch: 459/1000, Training Loss: 7.097377300262451, Validation Loss: 7.122952461242676\n",
      "Epoch: 460/1000, Training Loss: 7.097368240356445, Validation Loss: 7.122944355010986\n",
      "Epoch: 461/1000, Training Loss: 7.0973591804504395, Validation Loss: 7.122934341430664\n",
      "Epoch: 462/1000, Training Loss: 7.097350597381592, Validation Loss: 7.122926235198975\n",
      "Epoch: 463/1000, Training Loss: 7.097342491149902, Validation Loss: 7.122918128967285\n",
      "Epoch: 464/1000, Training Loss: 7.0973334312438965, Validation Loss: 7.122908115386963\n",
      "Epoch: 465/1000, Training Loss: 7.097324848175049, Validation Loss: 7.122899055480957\n",
      "Epoch: 466/1000, Training Loss: 7.097316741943359, Validation Loss: 7.122890472412109\n",
      "Epoch: 467/1000, Training Loss: 7.097307205200195, Validation Loss: 7.122880458831787\n",
      "Epoch: 468/1000, Training Loss: 7.097299575805664, Validation Loss: 7.122872829437256\n",
      "Epoch: 469/1000, Training Loss: 7.097291469573975, Validation Loss: 7.122864246368408\n",
      "Epoch: 470/1000, Training Loss: 7.097282409667969, Validation Loss: 7.122854709625244\n",
      "Epoch: 471/1000, Training Loss: 7.097274303436279, Validation Loss: 7.122846603393555\n",
      "Epoch: 472/1000, Training Loss: 7.09726619720459, Validation Loss: 7.122838020324707\n",
      "Epoch: 473/1000, Training Loss: 7.097257614135742, Validation Loss: 7.122828960418701\n",
      "Epoch: 474/1000, Training Loss: 7.0972490310668945, Validation Loss: 7.1228203773498535\n",
      "Epoch: 475/1000, Training Loss: 7.097240924835205, Validation Loss: 7.122812271118164\n",
      "Epoch: 476/1000, Training Loss: 7.097232818603516, Validation Loss: 7.122803211212158\n",
      "Epoch: 477/1000, Training Loss: 7.097224712371826, Validation Loss: 7.1227946281433105\n",
      "Epoch: 478/1000, Training Loss: 7.097217082977295, Validation Loss: 7.122786521911621\n",
      "Epoch: 479/1000, Training Loss: 7.097207546234131, Validation Loss: 7.122777462005615\n",
      "Epoch: 480/1000, Training Loss: 7.0971999168396, Validation Loss: 7.122768402099609\n",
      "Epoch: 481/1000, Training Loss: 7.097192287445068, Validation Loss: 7.12276029586792\n",
      "Epoch: 482/1000, Training Loss: 7.097183704376221, Validation Loss: 7.1227521896362305\n",
      "Epoch: 483/1000, Training Loss: 7.0971760749816895, Validation Loss: 7.122743606567383\n",
      "Epoch: 484/1000, Training Loss: 7.09716796875, Validation Loss: 7.122735023498535\n",
      "Epoch: 485/1000, Training Loss: 7.0971598625183105, Validation Loss: 7.1227264404296875\n",
      "Epoch: 486/1000, Training Loss: 7.097151279449463, Validation Loss: 7.122718811035156\n",
      "Epoch: 487/1000, Training Loss: 7.09714412689209, Validation Loss: 7.122710227966309\n",
      "Epoch: 488/1000, Training Loss: 7.097135543823242, Validation Loss: 7.122701644897461\n",
      "Epoch: 489/1000, Training Loss: 7.0971269607543945, Validation Loss: 7.122693061828613\n",
      "Epoch: 490/1000, Training Loss: 7.0971198081970215, Validation Loss: 7.122684955596924\n",
      "Epoch: 491/1000, Training Loss: 7.097111225128174, Validation Loss: 7.122676849365234\n",
      "Epoch: 492/1000, Training Loss: 7.097103118896484, Validation Loss: 7.122668266296387\n",
      "Epoch: 493/1000, Training Loss: 7.097095966339111, Validation Loss: 7.122660160064697\n",
      "Epoch: 494/1000, Training Loss: 7.097087860107422, Validation Loss: 7.122652530670166\n",
      "Epoch: 495/1000, Training Loss: 7.097080707550049, Validation Loss: 7.122644424438477\n",
      "Epoch: 496/1000, Training Loss: 7.097072124481201, Validation Loss: 7.122635841369629\n",
      "Epoch: 497/1000, Training Loss: 7.0970635414123535, Validation Loss: 7.1226277351379395\n",
      "Epoch: 498/1000, Training Loss: 7.097056865692139, Validation Loss: 7.12261962890625\n",
      "Epoch: 499/1000, Training Loss: 7.097048282623291, Validation Loss: 7.122611045837402\n",
      "Epoch: 500/1000, Training Loss: 7.097041130065918, Validation Loss: 7.122603416442871\n",
      "Epoch: 501/1000, Training Loss: 7.0970330238342285, Validation Loss: 7.122594833374023\n",
      "Epoch: 502/1000, Training Loss: 7.097025394439697, Validation Loss: 7.122586727142334\n",
      "Epoch: 503/1000, Training Loss: 7.097017765045166, Validation Loss: 7.122578144073486\n",
      "Epoch: 504/1000, Training Loss: 7.097009658813477, Validation Loss: 7.122570514678955\n",
      "Epoch: 505/1000, Training Loss: 7.0970025062561035, Validation Loss: 7.122562408447266\n",
      "Epoch: 506/1000, Training Loss: 7.096994400024414, Validation Loss: 7.122553825378418\n",
      "Epoch: 507/1000, Training Loss: 7.096986293792725, Validation Loss: 7.122546672821045\n",
      "Epoch: 508/1000, Training Loss: 7.09697961807251, Validation Loss: 7.1225385665893555\n",
      "Epoch: 509/1000, Training Loss: 7.096971035003662, Validation Loss: 7.122530937194824\n",
      "Epoch: 510/1000, Training Loss: 7.096963405609131, Validation Loss: 7.122522354125977\n",
      "Epoch: 511/1000, Training Loss: 7.096956253051758, Validation Loss: 7.122514247894287\n",
      "Epoch: 512/1000, Training Loss: 7.096948623657227, Validation Loss: 7.122506618499756\n",
      "Epoch: 513/1000, Training Loss: 7.096940994262695, Validation Loss: 7.122498512268066\n",
      "Epoch: 514/1000, Training Loss: 7.096933364868164, Validation Loss: 7.122490882873535\n",
      "Epoch: 515/1000, Training Loss: 7.096925258636475, Validation Loss: 7.122482776641846\n",
      "Epoch: 516/1000, Training Loss: 7.096917629241943, Validation Loss: 7.122474670410156\n",
      "Epoch: 517/1000, Training Loss: 7.09691047668457, Validation Loss: 7.122467041015625\n",
      "Epoch: 518/1000, Training Loss: 7.096903324127197, Validation Loss: 7.122459411621094\n",
      "Epoch: 519/1000, Training Loss: 7.096895694732666, Validation Loss: 7.122451305389404\n",
      "Epoch: 520/1000, Training Loss: 7.096888065338135, Validation Loss: 7.122443675994873\n",
      "Epoch: 521/1000, Training Loss: 7.0968804359436035, Validation Loss: 7.1224365234375\n",
      "Epoch: 522/1000, Training Loss: 7.0968732833862305, Validation Loss: 7.122427940368652\n",
      "Epoch: 523/1000, Training Loss: 7.096865653991699, Validation Loss: 7.122420787811279\n",
      "Epoch: 524/1000, Training Loss: 7.096859455108643, Validation Loss: 7.12241268157959\n",
      "Epoch: 525/1000, Training Loss: 7.096851348876953, Validation Loss: 7.122405052185059\n",
      "Epoch: 526/1000, Training Loss: 7.09684419631958, Validation Loss: 7.122397422790527\n",
      "Epoch: 527/1000, Training Loss: 7.096837043762207, Validation Loss: 7.122389793395996\n",
      "Epoch: 528/1000, Training Loss: 7.096829414367676, Validation Loss: 7.122381687164307\n",
      "Epoch: 529/1000, Training Loss: 7.0968217849731445, Validation Loss: 7.122374534606934\n",
      "Epoch: 530/1000, Training Loss: 7.096814155578613, Validation Loss: 7.122366428375244\n",
      "Epoch: 531/1000, Training Loss: 7.096807956695557, Validation Loss: 7.122359275817871\n",
      "Epoch: 532/1000, Training Loss: 7.096799850463867, Validation Loss: 7.122352123260498\n",
      "Epoch: 533/1000, Training Loss: 7.096792697906494, Validation Loss: 7.122344017028809\n",
      "Epoch: 534/1000, Training Loss: 7.096785068511963, Validation Loss: 7.1223368644714355\n",
      "Epoch: 535/1000, Training Loss: 7.096777439117432, Validation Loss: 7.122329235076904\n",
      "Epoch: 536/1000, Training Loss: 7.096771240234375, Validation Loss: 7.122321128845215\n",
      "Epoch: 537/1000, Training Loss: 7.096763610839844, Validation Loss: 7.122313976287842\n",
      "Epoch: 538/1000, Training Loss: 7.096756935119629, Validation Loss: 7.122306823730469\n",
      "Epoch: 539/1000, Training Loss: 7.0967488288879395, Validation Loss: 7.122298717498779\n",
      "Epoch: 540/1000, Training Loss: 7.096742630004883, Validation Loss: 7.122291564941406\n",
      "Epoch: 541/1000, Training Loss: 7.096735000610352, Validation Loss: 7.122283935546875\n",
      "Epoch: 542/1000, Training Loss: 7.0967278480529785, Validation Loss: 7.12227725982666\n",
      "Epoch: 543/1000, Training Loss: 7.0967206954956055, Validation Loss: 7.122269153594971\n",
      "Epoch: 544/1000, Training Loss: 7.096714019775391, Validation Loss: 7.1222615242004395\n",
      "Epoch: 545/1000, Training Loss: 7.096706867218018, Validation Loss: 7.122253894805908\n",
      "Epoch: 546/1000, Training Loss: 7.0966997146606445, Validation Loss: 7.122247219085693\n",
      "Epoch: 547/1000, Training Loss: 7.09669303894043, Validation Loss: 7.122239589691162\n",
      "Epoch: 548/1000, Training Loss: 7.096685409545898, Validation Loss: 7.122232437133789\n",
      "Epoch: 549/1000, Training Loss: 7.096678733825684, Validation Loss: 7.122225284576416\n",
      "Epoch: 550/1000, Training Loss: 7.0966715812683105, Validation Loss: 7.122217178344727\n",
      "Epoch: 551/1000, Training Loss: 7.096664905548096, Validation Loss: 7.12221097946167\n",
      "Epoch: 552/1000, Training Loss: 7.096657752990723, Validation Loss: 7.122203826904297\n",
      "Epoch: 553/1000, Training Loss: 7.096650123596191, Validation Loss: 7.122196674346924\n",
      "Epoch: 554/1000, Training Loss: 7.096643447875977, Validation Loss: 7.122189521789551\n",
      "Epoch: 555/1000, Training Loss: 7.09663724899292, Validation Loss: 7.122180938720703\n",
      "Epoch: 556/1000, Training Loss: 7.096629619598389, Validation Loss: 7.122174263000488\n",
      "Epoch: 557/1000, Training Loss: 7.096622467041016, Validation Loss: 7.122167110443115\n",
      "Epoch: 558/1000, Training Loss: 7.096616744995117, Validation Loss: 7.122159957885742\n",
      "Epoch: 559/1000, Training Loss: 7.096609592437744, Validation Loss: 7.122152805328369\n",
      "Epoch: 560/1000, Training Loss: 7.096602439880371, Validation Loss: 7.122145652770996\n",
      "Epoch: 561/1000, Training Loss: 7.096595764160156, Validation Loss: 7.122138977050781\n",
      "Epoch: 562/1000, Training Loss: 7.096588134765625, Validation Loss: 7.12213134765625\n",
      "Epoch: 563/1000, Training Loss: 7.096581935882568, Validation Loss: 7.122124195098877\n",
      "Epoch: 564/1000, Training Loss: 7.0965752601623535, Validation Loss: 7.122117042541504\n",
      "Epoch: 565/1000, Training Loss: 7.096568584442139, Validation Loss: 7.122110366821289\n",
      "Epoch: 566/1000, Training Loss: 7.096561431884766, Validation Loss: 7.122103214263916\n",
      "Epoch: 567/1000, Training Loss: 7.096555233001709, Validation Loss: 7.122096061706543\n",
      "Epoch: 568/1000, Training Loss: 7.096548557281494, Validation Loss: 7.122089385986328\n",
      "Epoch: 569/1000, Training Loss: 7.096541404724121, Validation Loss: 7.122082233428955\n",
      "Epoch: 570/1000, Training Loss: 7.096534729003906, Validation Loss: 7.122075080871582\n",
      "Epoch: 571/1000, Training Loss: 7.096528053283691, Validation Loss: 7.122068881988525\n",
      "Epoch: 572/1000, Training Loss: 7.096520900726318, Validation Loss: 7.122060775756836\n",
      "Epoch: 573/1000, Training Loss: 7.096514701843262, Validation Loss: 7.122054576873779\n",
      "Epoch: 574/1000, Training Loss: 7.096508502960205, Validation Loss: 7.122046947479248\n",
      "Epoch: 575/1000, Training Loss: 7.09650182723999, Validation Loss: 7.122039794921875\n",
      "Epoch: 576/1000, Training Loss: 7.096494674682617, Validation Loss: 7.122033596038818\n",
      "Epoch: 577/1000, Training Loss: 7.095804691314697, Validation Loss: 7.122026443481445\n",
      "Epoch: 578/1000, Training Loss: 7.095791816711426, Validation Loss: 7.1220197677612305\n",
      "Epoch: 579/1000, Training Loss: 7.095778942108154, Validation Loss: 7.122012615203857\n",
      "Epoch: 580/1000, Training Loss: 7.095765113830566, Validation Loss: 7.122005939483643\n",
      "Epoch: 581/1000, Training Loss: 7.095751762390137, Validation Loss: 7.121999263763428\n",
      "Epoch: 582/1000, Training Loss: 7.095739841461182, Validation Loss: 7.121992111206055\n",
      "Epoch: 583/1000, Training Loss: 7.0957255363464355, Validation Loss: 7.121984958648682\n",
      "Epoch: 584/1000, Training Loss: 7.095712661743164, Validation Loss: 7.121978759765625\n",
      "Epoch: 585/1000, Training Loss: 7.095700263977051, Validation Loss: 7.12197208404541\n",
      "Epoch: 586/1000, Training Loss: 7.095687389373779, Validation Loss: 7.1219658851623535\n",
      "Epoch: 587/1000, Training Loss: 7.09567403793335, Validation Loss: 7.1219587326049805\n",
      "Epoch: 588/1000, Training Loss: 7.09566068649292, Validation Loss: 7.121952056884766\n",
      "Epoch: 589/1000, Training Loss: 7.095648765563965, Validation Loss: 7.121945381164551\n",
      "Epoch: 590/1000, Training Loss: 7.095635414123535, Validation Loss: 7.121938705444336\n",
      "Epoch: 591/1000, Training Loss: 7.095623016357422, Validation Loss: 7.121932029724121\n",
      "Epoch: 592/1000, Training Loss: 7.095609664916992, Validation Loss: 7.121925354003906\n",
      "Epoch: 593/1000, Training Loss: 7.095597267150879, Validation Loss: 7.121918678283691\n",
      "Epoch: 594/1000, Training Loss: 7.095584392547607, Validation Loss: 7.121911525726318\n",
      "Epoch: 595/1000, Training Loss: 7.095571041107178, Validation Loss: 7.12190580368042\n",
      "Epoch: 596/1000, Training Loss: 7.0955586433410645, Validation Loss: 7.121898651123047\n",
      "Epoch: 597/1000, Training Loss: 7.095546245574951, Validation Loss: 7.121891975402832\n",
      "Epoch: 598/1000, Training Loss: 7.095533847808838, Validation Loss: 7.121885776519775\n",
      "Epoch: 599/1000, Training Loss: 7.095521450042725, Validation Loss: 7.1218791007995605\n",
      "Epoch: 600/1000, Training Loss: 7.095508575439453, Validation Loss: 7.121872425079346\n",
      "Epoch: 601/1000, Training Loss: 7.09549617767334, Validation Loss: 7.121866703033447\n",
      "Epoch: 602/1000, Training Loss: 7.095483779907227, Validation Loss: 7.121860027313232\n",
      "Epoch: 603/1000, Training Loss: 7.095470905303955, Validation Loss: 7.121853828430176\n",
      "Epoch: 604/1000, Training Loss: 7.095459461212158, Validation Loss: 7.121847152709961\n",
      "Epoch: 605/1000, Training Loss: 7.095447063446045, Validation Loss: 7.121840476989746\n",
      "Epoch: 606/1000, Training Loss: 7.095434188842773, Validation Loss: 7.121833801269531\n",
      "Epoch: 607/1000, Training Loss: 7.09542179107666, Validation Loss: 7.121828556060791\n",
      "Epoch: 608/1000, Training Loss: 7.095410346984863, Validation Loss: 7.121821880340576\n",
      "Epoch: 609/1000, Training Loss: 7.095397472381592, Validation Loss: 7.121815204620361\n",
      "Epoch: 610/1000, Training Loss: 7.095385551452637, Validation Loss: 7.121809005737305\n",
      "Epoch: 611/1000, Training Loss: 7.095373153686523, Validation Loss: 7.121802806854248\n",
      "Epoch: 612/1000, Training Loss: 7.095361232757568, Validation Loss: 7.121796607971191\n",
      "Epoch: 613/1000, Training Loss: 7.095348358154297, Validation Loss: 7.121789932250977\n",
      "Epoch: 614/1000, Training Loss: 7.095336437225342, Validation Loss: 7.12178373336792\n",
      "Epoch: 615/1000, Training Loss: 7.095324516296387, Validation Loss: 7.121777057647705\n",
      "Epoch: 616/1000, Training Loss: 7.095312595367432, Validation Loss: 7.121770858764648\n",
      "Epoch: 617/1000, Training Loss: 7.095300197601318, Validation Loss: 7.121764659881592\n",
      "Epoch: 618/1000, Training Loss: 7.0952887535095215, Validation Loss: 7.121758460998535\n",
      "Epoch: 619/1000, Training Loss: 7.09527587890625, Validation Loss: 7.12175178527832\n",
      "Epoch: 620/1000, Training Loss: 7.095264434814453, Validation Loss: 7.121746063232422\n",
      "Epoch: 621/1000, Training Loss: 7.095252990722656, Validation Loss: 7.121739387512207\n",
      "Epoch: 622/1000, Training Loss: 7.095240592956543, Validation Loss: 7.121733665466309\n",
      "Epoch: 623/1000, Training Loss: 7.095228672027588, Validation Loss: 7.121726989746094\n",
      "Epoch: 624/1000, Training Loss: 7.095216751098633, Validation Loss: 7.1217217445373535\n",
      "Epoch: 625/1000, Training Loss: 7.095205307006836, Validation Loss: 7.121715068817139\n",
      "Epoch: 626/1000, Training Loss: 7.095193386077881, Validation Loss: 7.12170934677124\n",
      "Epoch: 627/1000, Training Loss: 7.095181941986084, Validation Loss: 7.121703624725342\n",
      "Epoch: 628/1000, Training Loss: 7.095170497894287, Validation Loss: 7.121697425842285\n",
      "Epoch: 629/1000, Training Loss: 7.095158100128174, Validation Loss: 7.1216912269592285\n",
      "Epoch: 630/1000, Training Loss: 7.095146656036377, Validation Loss: 7.121684551239014\n",
      "Epoch: 631/1000, Training Loss: 7.095134735107422, Validation Loss: 7.121678352355957\n",
      "Epoch: 632/1000, Training Loss: 7.095123291015625, Validation Loss: 7.1216721534729\n",
      "Epoch: 633/1000, Training Loss: 7.09511137008667, Validation Loss: 7.121666431427002\n",
      "Epoch: 634/1000, Training Loss: 7.095099925994873, Validation Loss: 7.121661186218262\n",
      "Epoch: 635/1000, Training Loss: 7.095088005065918, Validation Loss: 7.121654987335205\n",
      "Epoch: 636/1000, Training Loss: 7.095077037811279, Validation Loss: 7.121648788452148\n",
      "Epoch: 637/1000, Training Loss: 7.095065593719482, Validation Loss: 7.121642112731934\n",
      "Epoch: 638/1000, Training Loss: 7.0950541496276855, Validation Loss: 7.121636867523193\n",
      "Epoch: 639/1000, Training Loss: 7.095042705535889, Validation Loss: 7.121630668640137\n",
      "Epoch: 640/1000, Training Loss: 7.095031261444092, Validation Loss: 7.121624946594238\n",
      "Epoch: 641/1000, Training Loss: 7.095019340515137, Validation Loss: 7.121618747711182\n",
      "Epoch: 642/1000, Training Loss: 7.095008373260498, Validation Loss: 7.121612071990967\n",
      "Epoch: 643/1000, Training Loss: 7.094996929168701, Validation Loss: 7.121607303619385\n",
      "Epoch: 644/1000, Training Loss: 7.094985485076904, Validation Loss: 7.121601581573486\n",
      "Epoch: 645/1000, Training Loss: 7.094975471496582, Validation Loss: 7.12159538269043\n",
      "Epoch: 646/1000, Training Loss: 7.094963550567627, Validation Loss: 7.121589660644531\n",
      "Epoch: 647/1000, Training Loss: 7.094952583312988, Validation Loss: 7.121583938598633\n",
      "Epoch: 648/1000, Training Loss: 7.094941139221191, Validation Loss: 7.121577739715576\n",
      "Epoch: 649/1000, Training Loss: 7.094930648803711, Validation Loss: 7.121572017669678\n",
      "Epoch: 650/1000, Training Loss: 7.094919681549072, Validation Loss: 7.121566295623779\n",
      "Epoch: 651/1000, Training Loss: 7.094908237457275, Validation Loss: 7.121561050415039\n",
      "Epoch: 652/1000, Training Loss: 7.0948967933654785, Validation Loss: 7.121555328369141\n",
      "Epoch: 653/1000, Training Loss: 7.09488582611084, Validation Loss: 7.121549129486084\n",
      "Epoch: 654/1000, Training Loss: 7.094875335693359, Validation Loss: 7.1215434074401855\n",
      "Epoch: 655/1000, Training Loss: 7.0948638916015625, Validation Loss: 7.121537208557129\n",
      "Epoch: 656/1000, Training Loss: 7.094852924346924, Validation Loss: 7.121531009674072\n",
      "Epoch: 657/1000, Training Loss: 7.094841957092285, Validation Loss: 7.121526718139648\n",
      "Epoch: 658/1000, Training Loss: 7.094831943511963, Validation Loss: 7.12152099609375\n",
      "Epoch: 659/1000, Training Loss: 7.094820022583008, Validation Loss: 7.12151575088501\n",
      "Epoch: 660/1000, Training Loss: 7.094809532165527, Validation Loss: 7.121509552001953\n",
      "Epoch: 661/1000, Training Loss: 7.0947980880737305, Validation Loss: 7.121504306793213\n",
      "Epoch: 662/1000, Training Loss: 7.09478759765625, Validation Loss: 7.1214985847473145\n",
      "Epoch: 663/1000, Training Loss: 7.094777584075928, Validation Loss: 7.121492385864258\n",
      "Epoch: 664/1000, Training Loss: 7.094766139984131, Validation Loss: 7.121487617492676\n",
      "Epoch: 665/1000, Training Loss: 7.094756126403809, Validation Loss: 7.121480941772461\n",
      "Epoch: 666/1000, Training Loss: 7.09474515914917, Validation Loss: 7.1214752197265625\n",
      "Epoch: 667/1000, Training Loss: 7.094735145568848, Validation Loss: 7.1214704513549805\n",
      "Epoch: 668/1000, Training Loss: 7.094723701477051, Validation Loss: 7.121464252471924\n",
      "Epoch: 669/1000, Training Loss: 7.09471321105957, Validation Loss: 7.121459484100342\n",
      "Epoch: 670/1000, Training Loss: 7.09470272064209, Validation Loss: 7.121453762054443\n",
      "Epoch: 671/1000, Training Loss: 7.094692707061768, Validation Loss: 7.121448516845703\n",
      "Epoch: 672/1000, Training Loss: 7.094682216644287, Validation Loss: 7.121442794799805\n",
      "Epoch: 673/1000, Training Loss: 7.094671726226807, Validation Loss: 7.1214375495910645\n",
      "Epoch: 674/1000, Training Loss: 7.094661712646484, Validation Loss: 7.121431827545166\n",
      "Epoch: 675/1000, Training Loss: 7.094650745391846, Validation Loss: 7.121427536010742\n",
      "Epoch: 676/1000, Training Loss: 7.094639778137207, Validation Loss: 7.121421813964844\n",
      "Epoch: 677/1000, Training Loss: 7.094630241394043, Validation Loss: 7.1214165687561035\n",
      "Epoch: 678/1000, Training Loss: 7.0946197509765625, Validation Loss: 7.121410846710205\n",
      "Epoch: 679/1000, Training Loss: 7.094609260559082, Validation Loss: 7.121405601501465\n",
      "Epoch: 680/1000, Training Loss: 7.094598770141602, Validation Loss: 7.121400356292725\n",
      "Epoch: 681/1000, Training Loss: 7.094589710235596, Validation Loss: 7.121395587921143\n",
      "Epoch: 682/1000, Training Loss: 7.094579219818115, Validation Loss: 7.121388912200928\n",
      "Epoch: 683/1000, Training Loss: 7.094568729400635, Validation Loss: 7.1213836669921875\n",
      "Epoch: 684/1000, Training Loss: 7.0945587158203125, Validation Loss: 7.121378421783447\n",
      "Epoch: 685/1000, Training Loss: 7.094548225402832, Validation Loss: 7.121374130249023\n",
      "Epoch: 686/1000, Training Loss: 7.09453821182251, Validation Loss: 7.121368408203125\n",
      "Epoch: 687/1000, Training Loss: 7.094528675079346, Validation Loss: 7.121363162994385\n",
      "Epoch: 688/1000, Training Loss: 7.094518184661865, Validation Loss: 7.121357440948486\n",
      "Epoch: 689/1000, Training Loss: 7.094507694244385, Validation Loss: 7.121352672576904\n",
      "Epoch: 690/1000, Training Loss: 7.094498634338379, Validation Loss: 7.121347904205322\n",
      "Epoch: 691/1000, Training Loss: 7.094488620758057, Validation Loss: 7.121342658996582\n",
      "Epoch: 692/1000, Training Loss: 7.094478607177734, Validation Loss: 7.121337413787842\n",
      "Epoch: 693/1000, Training Loss: 7.094468116760254, Validation Loss: 7.121331691741943\n",
      "Epoch: 694/1000, Training Loss: 7.09445858001709, Validation Loss: 7.1213274002075195\n",
      "Epoch: 695/1000, Training Loss: 7.094449043273926, Validation Loss: 7.121321678161621\n",
      "Epoch: 696/1000, Training Loss: 7.09443998336792, Validation Loss: 7.121316909790039\n",
      "Epoch: 697/1000, Training Loss: 7.094429016113281, Validation Loss: 7.121311187744141\n",
      "Epoch: 698/1000, Training Loss: 7.094419002532959, Validation Loss: 7.121306896209717\n",
      "Epoch: 699/1000, Training Loss: 7.094409465789795, Validation Loss: 7.121302127838135\n",
      "Epoch: 700/1000, Training Loss: 7.094400405883789, Validation Loss: 7.121296405792236\n",
      "Epoch: 701/1000, Training Loss: 7.094390869140625, Validation Loss: 7.121291160583496\n",
      "Epoch: 702/1000, Training Loss: 7.094380855560303, Validation Loss: 7.121286392211914\n",
      "Epoch: 703/1000, Training Loss: 7.0943708419799805, Validation Loss: 7.121281623840332\n",
      "Epoch: 704/1000, Training Loss: 7.094361305236816, Validation Loss: 7.121275901794434\n",
      "Epoch: 705/1000, Training Loss: 7.094351768493652, Validation Loss: 7.121271133422852\n",
      "Epoch: 706/1000, Training Loss: 7.0943427085876465, Validation Loss: 7.1212663650512695\n",
      "Epoch: 707/1000, Training Loss: 7.094332695007324, Validation Loss: 7.121261119842529\n",
      "Epoch: 708/1000, Training Loss: 7.09432315826416, Validation Loss: 7.121256351470947\n",
      "Epoch: 709/1000, Training Loss: 7.094313144683838, Validation Loss: 7.121251106262207\n",
      "Epoch: 710/1000, Training Loss: 7.09430456161499, Validation Loss: 7.121246337890625\n",
      "Epoch: 711/1000, Training Loss: 7.094294548034668, Validation Loss: 7.121241569519043\n",
      "Epoch: 712/1000, Training Loss: 7.094285011291504, Validation Loss: 7.121236324310303\n",
      "Epoch: 713/1000, Training Loss: 7.094275951385498, Validation Loss: 7.121231555938721\n",
      "Epoch: 714/1000, Training Loss: 7.094265460968018, Validation Loss: 7.1212263107299805\n",
      "Epoch: 715/1000, Training Loss: 7.09425687789917, Validation Loss: 7.121222496032715\n",
      "Epoch: 716/1000, Training Loss: 7.094247817993164, Validation Loss: 7.121217727661133\n",
      "Epoch: 717/1000, Training Loss: 7.09423828125, Validation Loss: 7.121212959289551\n",
      "Epoch: 718/1000, Training Loss: 7.094228267669678, Validation Loss: 7.1212077140808105\n",
      "Epoch: 719/1000, Training Loss: 7.094219207763672, Validation Loss: 7.1212029457092285\n",
      "Epoch: 720/1000, Training Loss: 7.094210624694824, Validation Loss: 7.121197700500488\n",
      "Epoch: 721/1000, Training Loss: 7.094201564788818, Validation Loss: 7.1211934089660645\n",
      "Epoch: 722/1000, Training Loss: 7.094192028045654, Validation Loss: 7.121188640594482\n",
      "Epoch: 723/1000, Training Loss: 7.094182968139648, Validation Loss: 7.1211838722229\n",
      "Epoch: 724/1000, Training Loss: 7.094173908233643, Validation Loss: 7.12117862701416\n",
      "Epoch: 725/1000, Training Loss: 7.0941643714904785, Validation Loss: 7.121174335479736\n",
      "Epoch: 726/1000, Training Loss: 7.094155788421631, Validation Loss: 7.121169090270996\n",
      "Epoch: 727/1000, Training Loss: 7.094146728515625, Validation Loss: 7.121164321899414\n",
      "Epoch: 728/1000, Training Loss: 7.094137668609619, Validation Loss: 7.12116003036499\n",
      "Epoch: 729/1000, Training Loss: 7.094128608703613, Validation Loss: 7.121155261993408\n",
      "Epoch: 730/1000, Training Loss: 7.094120025634766, Validation Loss: 7.121150493621826\n",
      "Epoch: 731/1000, Training Loss: 7.09411096572876, Validation Loss: 7.1211466789245605\n",
      "Epoch: 732/1000, Training Loss: 7.094102382659912, Validation Loss: 7.121142387390137\n",
      "Epoch: 733/1000, Training Loss: 7.094092845916748, Validation Loss: 7.121136665344238\n",
      "Epoch: 734/1000, Training Loss: 7.094083786010742, Validation Loss: 7.1211323738098145\n",
      "Epoch: 735/1000, Training Loss: 7.094075679779053, Validation Loss: 7.121127128601074\n",
      "Epoch: 736/1000, Training Loss: 7.094066143035889, Validation Loss: 7.12112283706665\n",
      "Epoch: 737/1000, Training Loss: 7.094058513641357, Validation Loss: 7.121118068695068\n",
      "Epoch: 738/1000, Training Loss: 7.094048976898193, Validation Loss: 7.1211137771606445\n",
      "Epoch: 739/1000, Training Loss: 7.0940399169921875, Validation Loss: 7.1211090087890625\n",
      "Epoch: 740/1000, Training Loss: 7.094030857086182, Validation Loss: 7.121104717254639\n",
      "Epoch: 741/1000, Training Loss: 7.094022274017334, Validation Loss: 7.121100425720215\n",
      "Epoch: 742/1000, Training Loss: 7.0940141677856445, Validation Loss: 7.121096134185791\n",
      "Epoch: 743/1000, Training Loss: 7.094005584716797, Validation Loss: 7.121090888977051\n",
      "Epoch: 744/1000, Training Loss: 7.093997478485107, Validation Loss: 7.121087074279785\n",
      "Epoch: 745/1000, Training Loss: 7.093988418579102, Validation Loss: 7.121082305908203\n",
      "Epoch: 746/1000, Training Loss: 7.093979835510254, Validation Loss: 7.121077060699463\n",
      "Epoch: 747/1000, Training Loss: 7.093970775604248, Validation Loss: 7.121073246002197\n",
      "Epoch: 748/1000, Training Loss: 7.093962669372559, Validation Loss: 7.121068954467773\n",
      "Epoch: 749/1000, Training Loss: 7.093954086303711, Validation Loss: 7.121064186096191\n",
      "Epoch: 750/1000, Training Loss: 7.093945503234863, Validation Loss: 7.121060371398926\n",
      "Epoch: 751/1000, Training Loss: 7.093937873840332, Validation Loss: 7.121055603027344\n",
      "Epoch: 752/1000, Training Loss: 7.093928337097168, Validation Loss: 7.121050834655762\n",
      "Epoch: 753/1000, Training Loss: 7.093920707702637, Validation Loss: 7.121047496795654\n",
      "Epoch: 754/1000, Training Loss: 7.093912124633789, Validation Loss: 7.121042728424072\n",
      "Epoch: 755/1000, Training Loss: 7.093903064727783, Validation Loss: 7.121037483215332\n",
      "Epoch: 756/1000, Training Loss: 7.093895435333252, Validation Loss: 7.121034622192383\n",
      "Epoch: 757/1000, Training Loss: 7.093886375427246, Validation Loss: 7.121029853820801\n",
      "Epoch: 758/1000, Training Loss: 7.093878746032715, Validation Loss: 7.121025562286377\n",
      "Epoch: 759/1000, Training Loss: 7.093871116638184, Validation Loss: 7.121021270751953\n",
      "Epoch: 760/1000, Training Loss: 7.093862533569336, Validation Loss: 7.121016502380371\n",
      "Epoch: 761/1000, Training Loss: 7.0938544273376465, Validation Loss: 7.1210126876831055\n",
      "Epoch: 762/1000, Training Loss: 7.093846797943115, Validation Loss: 7.121008396148682\n",
      "Epoch: 763/1000, Training Loss: 7.093838214874268, Validation Loss: 7.121004581451416\n",
      "Epoch: 764/1000, Training Loss: 7.093830108642578, Validation Loss: 7.121000289916992\n",
      "Epoch: 765/1000, Training Loss: 7.093822479248047, Validation Loss: 7.12099552154541\n",
      "Epoch: 766/1000, Training Loss: 7.093814373016357, Validation Loss: 7.1209917068481445\n",
      "Epoch: 767/1000, Training Loss: 7.09380578994751, Validation Loss: 7.120987415313721\n",
      "Epoch: 768/1000, Training Loss: 7.09379768371582, Validation Loss: 7.120983600616455\n",
      "Epoch: 769/1000, Training Loss: 7.093790054321289, Validation Loss: 7.1209797859191895\n",
      "Epoch: 770/1000, Training Loss: 7.0937819480896, Validation Loss: 7.120974540710449\n",
      "Epoch: 771/1000, Training Loss: 7.093774318695068, Validation Loss: 7.120970726013184\n",
      "Epoch: 772/1000, Training Loss: 7.093766212463379, Validation Loss: 7.12096643447876\n",
      "Epoch: 773/1000, Training Loss: 7.0937581062316895, Validation Loss: 7.120962619781494\n",
      "Epoch: 774/1000, Training Loss: 7.093750953674316, Validation Loss: 7.1209588050842285\n",
      "Epoch: 775/1000, Training Loss: 7.093742370605469, Validation Loss: 7.120954513549805\n",
      "Epoch: 776/1000, Training Loss: 7.093734264373779, Validation Loss: 7.120950222015381\n",
      "Epoch: 777/1000, Training Loss: 7.093726634979248, Validation Loss: 7.120945930480957\n",
      "Epoch: 778/1000, Training Loss: 7.093718528747559, Validation Loss: 7.120942115783691\n",
      "Epoch: 779/1000, Training Loss: 7.0937113761901855, Validation Loss: 7.120938301086426\n",
      "Epoch: 780/1000, Training Loss: 7.093703746795654, Validation Loss: 7.120934009552002\n",
      "Epoch: 781/1000, Training Loss: 7.093695640563965, Validation Loss: 7.1209306716918945\n",
      "Epoch: 782/1000, Training Loss: 7.093687534332275, Validation Loss: 7.120926380157471\n",
      "Epoch: 783/1000, Training Loss: 7.093680381774902, Validation Loss: 7.120922565460205\n",
      "Epoch: 784/1000, Training Loss: 7.093672752380371, Validation Loss: 7.1209187507629395\n",
      "Epoch: 785/1000, Training Loss: 7.093664646148682, Validation Loss: 7.120913982391357\n",
      "Epoch: 786/1000, Training Loss: 7.093657493591309, Validation Loss: 7.120909690856934\n",
      "Epoch: 787/1000, Training Loss: 7.093649864196777, Validation Loss: 7.120905876159668\n",
      "Epoch: 788/1000, Training Loss: 7.093642234802246, Validation Loss: 7.1209025382995605\n",
      "Epoch: 789/1000, Training Loss: 7.093634128570557, Validation Loss: 7.120898723602295\n",
      "Epoch: 790/1000, Training Loss: 7.093627452850342, Validation Loss: 7.120894432067871\n",
      "Epoch: 791/1000, Training Loss: 7.0936198234558105, Validation Loss: 7.120890140533447\n",
      "Epoch: 792/1000, Training Loss: 7.0936126708984375, Validation Loss: 7.120887279510498\n",
      "Epoch: 793/1000, Training Loss: 7.093605041503906, Validation Loss: 7.120882987976074\n",
      "Epoch: 794/1000, Training Loss: 7.093596935272217, Validation Loss: 7.120879650115967\n",
      "Epoch: 795/1000, Training Loss: 7.093589782714844, Validation Loss: 7.120874404907227\n",
      "Epoch: 796/1000, Training Loss: 7.0935821533203125, Validation Loss: 7.120871067047119\n",
      "Epoch: 797/1000, Training Loss: 7.093576431274414, Validation Loss: 7.1208672523498535\n",
      "Epoch: 798/1000, Training Loss: 7.093568325042725, Validation Loss: 7.120863437652588\n",
      "Epoch: 799/1000, Training Loss: 7.093560695648193, Validation Loss: 7.120859622955322\n",
      "Epoch: 800/1000, Training Loss: 7.09355354309082, Validation Loss: 7.120856285095215\n",
      "Epoch: 801/1000, Training Loss: 7.093546390533447, Validation Loss: 7.120852470397949\n",
      "Epoch: 802/1000, Training Loss: 7.093538761138916, Validation Loss: 7.120848178863525\n",
      "Epoch: 803/1000, Training Loss: 7.093531608581543, Validation Loss: 7.120844841003418\n",
      "Epoch: 804/1000, Training Loss: 7.09352445602417, Validation Loss: 7.120841026306152\n",
      "Epoch: 805/1000, Training Loss: 7.093517303466797, Validation Loss: 7.120837211608887\n",
      "Epoch: 806/1000, Training Loss: 7.093510150909424, Validation Loss: 7.120833396911621\n",
      "Epoch: 807/1000, Training Loss: 7.093502998352051, Validation Loss: 7.120830059051514\n",
      "Epoch: 808/1000, Training Loss: 7.093495845794678, Validation Loss: 7.120826721191406\n",
      "Epoch: 809/1000, Training Loss: 7.0934882164001465, Validation Loss: 7.120822429656982\n",
      "Epoch: 810/1000, Training Loss: 7.093481063842773, Validation Loss: 7.120819091796875\n",
      "Epoch: 811/1000, Training Loss: 7.093474388122559, Validation Loss: 7.120815277099609\n",
      "Epoch: 812/1000, Training Loss: 7.0934672355651855, Validation Loss: 7.120811939239502\n",
      "Epoch: 813/1000, Training Loss: 7.093461036682129, Validation Loss: 7.12080717086792\n",
      "Epoch: 814/1000, Training Loss: 7.093453884124756, Validation Loss: 7.1208038330078125\n",
      "Epoch: 815/1000, Training Loss: 7.093446254730225, Validation Loss: 7.120800495147705\n",
      "Epoch: 816/1000, Training Loss: 7.093439102172852, Validation Loss: 7.120796203613281\n",
      "Epoch: 817/1000, Training Loss: 7.093432903289795, Validation Loss: 7.120793342590332\n",
      "Epoch: 818/1000, Training Loss: 7.09342622756958, Validation Loss: 7.120789527893066\n",
      "Epoch: 819/1000, Training Loss: 7.093419551849365, Validation Loss: 7.120786190032959\n",
      "Epoch: 820/1000, Training Loss: 7.093412399291992, Validation Loss: 7.120782375335693\n",
      "Epoch: 821/1000, Training Loss: 7.093405246734619, Validation Loss: 7.120779991149902\n",
      "Epoch: 822/1000, Training Loss: 7.093398571014404, Validation Loss: 7.12077522277832\n",
      "Epoch: 823/1000, Training Loss: 7.093390941619873, Validation Loss: 7.120771884918213\n",
      "Epoch: 824/1000, Training Loss: 7.093384742736816, Validation Loss: 7.1207685470581055\n",
      "Epoch: 825/1000, Training Loss: 7.093378067016602, Validation Loss: 7.120765686035156\n",
      "Epoch: 826/1000, Training Loss: 7.093371391296387, Validation Loss: 7.120761394500732\n",
      "Epoch: 827/1000, Training Loss: 7.093364238739014, Validation Loss: 7.1207594871521\n",
      "Epoch: 828/1000, Training Loss: 7.093358516693115, Validation Loss: 7.120754241943359\n",
      "Epoch: 829/1000, Training Loss: 7.093351364135742, Validation Loss: 7.120751857757568\n",
      "Epoch: 830/1000, Training Loss: 7.093344688415527, Validation Loss: 7.120748519897461\n",
      "Epoch: 831/1000, Training Loss: 7.093339443206787, Validation Loss: 7.120744705200195\n",
      "Epoch: 832/1000, Training Loss: 7.093331336975098, Validation Loss: 7.120741367340088\n",
      "Epoch: 833/1000, Training Loss: 7.093325138092041, Validation Loss: 7.1207380294799805\n",
      "Epoch: 834/1000, Training Loss: 7.093318462371826, Validation Loss: 7.120734214782715\n",
      "Epoch: 835/1000, Training Loss: 7.0933122634887695, Validation Loss: 7.120731353759766\n",
      "Epoch: 836/1000, Training Loss: 7.093306064605713, Validation Loss: 7.120728015899658\n",
      "Epoch: 837/1000, Training Loss: 7.093299388885498, Validation Loss: 7.120724678039551\n",
      "Epoch: 838/1000, Training Loss: 7.093292236328125, Validation Loss: 7.120721340179443\n",
      "Epoch: 839/1000, Training Loss: 7.093286991119385, Validation Loss: 7.120718479156494\n",
      "Epoch: 840/1000, Training Loss: 7.093279838562012, Validation Loss: 7.12071418762207\n",
      "Epoch: 841/1000, Training Loss: 7.093274116516113, Validation Loss: 7.120710849761963\n",
      "Epoch: 842/1000, Training Loss: 7.093267440795898, Validation Loss: 7.120707988739014\n",
      "Epoch: 843/1000, Training Loss: 7.093260765075684, Validation Loss: 7.120704650878906\n",
      "Epoch: 844/1000, Training Loss: 7.093255043029785, Validation Loss: 7.120700836181641\n",
      "Epoch: 845/1000, Training Loss: 7.09324836730957, Validation Loss: 7.12069845199585\n",
      "Epoch: 846/1000, Training Loss: 7.093242645263672, Validation Loss: 7.120694637298584\n",
      "Epoch: 847/1000, Training Loss: 7.093235969543457, Validation Loss: 7.120691776275635\n",
      "Epoch: 848/1000, Training Loss: 7.093230247497559, Validation Loss: 7.1206889152526855\n",
      "Epoch: 849/1000, Training Loss: 7.093223571777344, Validation Loss: 7.120685577392578\n",
      "Epoch: 850/1000, Training Loss: 7.093217372894287, Validation Loss: 7.120681285858154\n",
      "Epoch: 851/1000, Training Loss: 7.093210697174072, Validation Loss: 7.120678901672363\n",
      "Epoch: 852/1000, Training Loss: 7.093204975128174, Validation Loss: 7.120675086975098\n",
      "Epoch: 853/1000, Training Loss: 7.093198299407959, Validation Loss: 7.120672225952148\n",
      "Epoch: 854/1000, Training Loss: 7.093193054199219, Validation Loss: 7.120668888092041\n",
      "Epoch: 855/1000, Training Loss: 7.093186855316162, Validation Loss: 7.12066650390625\n",
      "Epoch: 856/1000, Training Loss: 7.093181133270264, Validation Loss: 7.120663166046143\n",
      "Epoch: 857/1000, Training Loss: 7.093174934387207, Validation Loss: 7.120659828186035\n",
      "Epoch: 858/1000, Training Loss: 7.09316873550415, Validation Loss: 7.120657444000244\n",
      "Epoch: 859/1000, Training Loss: 7.093162536621094, Validation Loss: 7.1206536293029785\n",
      "Epoch: 860/1000, Training Loss: 7.093156337738037, Validation Loss: 7.120650768280029\n",
      "Epoch: 861/1000, Training Loss: 7.093150615692139, Validation Loss: 7.120647430419922\n",
      "Epoch: 862/1000, Training Loss: 7.09314489364624, Validation Loss: 7.1206440925598145\n",
      "Epoch: 863/1000, Training Loss: 7.093139171600342, Validation Loss: 7.120641708374023\n",
      "Epoch: 864/1000, Training Loss: 7.093132972717285, Validation Loss: 7.120638370513916\n",
      "Epoch: 865/1000, Training Loss: 7.0931267738342285, Validation Loss: 7.120635032653809\n",
      "Epoch: 866/1000, Training Loss: 7.09312105178833, Validation Loss: 7.120632648468018\n",
      "Epoch: 867/1000, Training Loss: 7.093114852905273, Validation Loss: 7.120628833770752\n",
      "Epoch: 868/1000, Training Loss: 7.093108654022217, Validation Loss: 7.1206254959106445\n",
      "Epoch: 869/1000, Training Loss: 7.093103408813477, Validation Loss: 7.120622634887695\n",
      "Epoch: 870/1000, Training Loss: 7.093098163604736, Validation Loss: 7.120620250701904\n",
      "Epoch: 871/1000, Training Loss: 7.0930914878845215, Validation Loss: 7.120616912841797\n",
      "Epoch: 872/1000, Training Loss: 7.0930867195129395, Validation Loss: 7.120614051818848\n",
      "Epoch: 873/1000, Training Loss: 7.093080520629883, Validation Loss: 7.120611667633057\n",
      "Epoch: 874/1000, Training Loss: 7.093074321746826, Validation Loss: 7.120607852935791\n",
      "Epoch: 875/1000, Training Loss: 7.093069076538086, Validation Loss: 7.120604991912842\n",
      "Epoch: 876/1000, Training Loss: 7.093062877655029, Validation Loss: 7.120601654052734\n",
      "Epoch: 877/1000, Training Loss: 7.093057155609131, Validation Loss: 7.120599269866943\n",
      "Epoch: 878/1000, Training Loss: 7.093052387237549, Validation Loss: 7.120596408843994\n",
      "Epoch: 879/1000, Training Loss: 7.09304666519165, Validation Loss: 7.120593547821045\n",
      "Epoch: 880/1000, Training Loss: 7.09304141998291, Validation Loss: 7.120591640472412\n",
      "Epoch: 881/1000, Training Loss: 7.0930352210998535, Validation Loss: 7.120588302612305\n",
      "Epoch: 882/1000, Training Loss: 7.093029975891113, Validation Loss: 7.120584964752197\n",
      "Epoch: 883/1000, Training Loss: 7.093024730682373, Validation Loss: 7.12058162689209\n",
      "Epoch: 884/1000, Training Loss: 7.093019008636475, Validation Loss: 7.120578765869141\n",
      "Epoch: 885/1000, Training Loss: 7.093012809753418, Validation Loss: 7.120577335357666\n",
      "Epoch: 886/1000, Training Loss: 7.093007564544678, Validation Loss: 7.120573997497559\n",
      "Epoch: 887/1000, Training Loss: 7.0930023193359375, Validation Loss: 7.120571613311768\n",
      "Epoch: 888/1000, Training Loss: 7.092997074127197, Validation Loss: 7.120567798614502\n",
      "Epoch: 889/1000, Training Loss: 7.092991828918457, Validation Loss: 7.120565891265869\n",
      "Epoch: 890/1000, Training Loss: 7.092986106872559, Validation Loss: 7.1205620765686035\n",
      "Epoch: 891/1000, Training Loss: 7.092980861663818, Validation Loss: 7.120560169219971\n",
      "Epoch: 892/1000, Training Loss: 7.092975616455078, Validation Loss: 7.1205573081970215\n",
      "Epoch: 893/1000, Training Loss: 7.092970371246338, Validation Loss: 7.120553970336914\n",
      "Epoch: 894/1000, Training Loss: 7.092964172363281, Validation Loss: 7.120551586151123\n",
      "Epoch: 895/1000, Training Loss: 7.092958927154541, Validation Loss: 7.120548248291016\n",
      "Epoch: 896/1000, Training Loss: 7.092954158782959, Validation Loss: 7.120545864105225\n",
      "Epoch: 897/1000, Training Loss: 7.092949390411377, Validation Loss: 7.120542526245117\n",
      "Epoch: 898/1000, Training Loss: 7.092944145202637, Validation Loss: 7.120540142059326\n",
      "Epoch: 899/1000, Training Loss: 7.092938423156738, Validation Loss: 7.120537281036377\n",
      "Epoch: 900/1000, Training Loss: 7.092933654785156, Validation Loss: 7.120534896850586\n",
      "Epoch: 901/1000, Training Loss: 7.092927932739258, Validation Loss: 7.120532512664795\n",
      "Epoch: 902/1000, Training Loss: 7.092922687530518, Validation Loss: 7.120528697967529\n",
      "Epoch: 903/1000, Training Loss: 7.092916965484619, Validation Loss: 7.1205267906188965\n",
      "Epoch: 904/1000, Training Loss: 7.092912197113037, Validation Loss: 7.1205244064331055\n",
      "Epoch: 905/1000, Training Loss: 7.092907428741455, Validation Loss: 7.120521545410156\n",
      "Epoch: 906/1000, Training Loss: 7.092903137207031, Validation Loss: 7.120519161224365\n",
      "Epoch: 907/1000, Training Loss: 7.092897415161133, Validation Loss: 7.120516300201416\n",
      "Epoch: 908/1000, Training Loss: 7.092893123626709, Validation Loss: 7.120514392852783\n",
      "Epoch: 909/1000, Training Loss: 7.092886924743652, Validation Loss: 7.120511531829834\n",
      "Epoch: 910/1000, Training Loss: 7.09288215637207, Validation Loss: 7.120508193969727\n",
      "Epoch: 911/1000, Training Loss: 7.092877388000488, Validation Loss: 7.120506763458252\n",
      "Epoch: 912/1000, Training Loss: 7.092872142791748, Validation Loss: 7.120503902435303\n",
      "Epoch: 913/1000, Training Loss: 7.092867374420166, Validation Loss: 7.1205010414123535\n",
      "Epoch: 914/1000, Training Loss: 7.092862606048584, Validation Loss: 7.120498180389404\n",
      "Epoch: 915/1000, Training Loss: 7.092857837677002, Validation Loss: 7.1204962730407715\n",
      "Epoch: 916/1000, Training Loss: 7.09285306930542, Validation Loss: 7.120493412017822\n",
      "Epoch: 917/1000, Training Loss: 7.092848300933838, Validation Loss: 7.1204915046691895\n",
      "Epoch: 918/1000, Training Loss: 7.092843055725098, Validation Loss: 7.120487689971924\n",
      "Epoch: 919/1000, Training Loss: 7.092837810516357, Validation Loss: 7.120486259460449\n",
      "Epoch: 920/1000, Training Loss: 7.092833518981934, Validation Loss: 7.1204833984375\n",
      "Epoch: 921/1000, Training Loss: 7.092828750610352, Validation Loss: 7.120481491088867\n",
      "Epoch: 922/1000, Training Loss: 7.092824459075928, Validation Loss: 7.120478630065918\n",
      "Epoch: 923/1000, Training Loss: 7.092818737030029, Validation Loss: 7.120476722717285\n",
      "Epoch: 924/1000, Training Loss: 7.092813968658447, Validation Loss: 7.120474338531494\n",
      "Epoch: 925/1000, Training Loss: 7.092809677124023, Validation Loss: 7.120471477508545\n",
      "Epoch: 926/1000, Training Loss: 7.092804908752441, Validation Loss: 7.120469093322754\n",
      "Epoch: 927/1000, Training Loss: 7.092801094055176, Validation Loss: 7.120466709136963\n",
      "Epoch: 928/1000, Training Loss: 7.092796325683594, Validation Loss: 7.120464324951172\n",
      "Epoch: 929/1000, Training Loss: 7.092791557312012, Validation Loss: 7.120461940765381\n",
      "Epoch: 930/1000, Training Loss: 7.09278678894043, Validation Loss: 7.12045955657959\n",
      "Epoch: 931/1000, Training Loss: 7.092782020568848, Validation Loss: 7.120457172393799\n",
      "Epoch: 932/1000, Training Loss: 7.092777252197266, Validation Loss: 7.120454788208008\n",
      "Epoch: 933/1000, Training Loss: 7.0927734375, Validation Loss: 7.120452404022217\n",
      "Epoch: 934/1000, Training Loss: 7.09276819229126, Validation Loss: 7.120450973510742\n",
      "Epoch: 935/1000, Training Loss: 7.092764377593994, Validation Loss: 7.120448112487793\n",
      "Epoch: 936/1000, Training Loss: 7.092759609222412, Validation Loss: 7.12044620513916\n",
      "Epoch: 937/1000, Training Loss: 7.09275484085083, Validation Loss: 7.120443820953369\n",
      "Epoch: 938/1000, Training Loss: 7.092750549316406, Validation Loss: 7.12044095993042\n",
      "Epoch: 939/1000, Training Loss: 7.092746734619141, Validation Loss: 7.120439052581787\n",
      "Epoch: 940/1000, Training Loss: 7.092742443084717, Validation Loss: 7.120436191558838\n",
      "Epoch: 941/1000, Training Loss: 7.092736721038818, Validation Loss: 7.120433807373047\n",
      "Epoch: 942/1000, Training Loss: 7.092732906341553, Validation Loss: 7.120432376861572\n",
      "Epoch: 943/1000, Training Loss: 7.092728614807129, Validation Loss: 7.1204304695129395\n",
      "Epoch: 944/1000, Training Loss: 7.092724323272705, Validation Loss: 7.12042760848999\n",
      "Epoch: 945/1000, Training Loss: 7.092720031738281, Validation Loss: 7.120425701141357\n",
      "Epoch: 946/1000, Training Loss: 7.092714786529541, Validation Loss: 7.120422840118408\n",
      "Epoch: 947/1000, Training Loss: 7.092711448669434, Validation Loss: 7.120420455932617\n",
      "Epoch: 948/1000, Training Loss: 7.092706203460693, Validation Loss: 7.120419025421143\n",
      "Epoch: 949/1000, Training Loss: 7.092702865600586, Validation Loss: 7.120416164398193\n",
      "Epoch: 950/1000, Training Loss: 7.092697620391846, Validation Loss: 7.120414733886719\n",
      "Epoch: 951/1000, Training Loss: 7.09269380569458, Validation Loss: 7.120412349700928\n",
      "Epoch: 952/1000, Training Loss: 7.0926899909973145, Validation Loss: 7.1204094886779785\n",
      "Epoch: 953/1000, Training Loss: 7.092685222625732, Validation Loss: 7.120408058166504\n",
      "Epoch: 954/1000, Training Loss: 7.092680931091309, Validation Loss: 7.120405197143555\n",
      "Epoch: 955/1000, Training Loss: 7.092676162719727, Validation Loss: 7.120404243469238\n",
      "Epoch: 956/1000, Training Loss: 7.092673301696777, Validation Loss: 7.120401382446289\n",
      "Epoch: 957/1000, Training Loss: 7.0926690101623535, Validation Loss: 7.120398998260498\n",
      "Epoch: 958/1000, Training Loss: 7.09266471862793, Validation Loss: 7.120397090911865\n",
      "Epoch: 959/1000, Training Loss: 7.092660903930664, Validation Loss: 7.120394706726074\n",
      "Epoch: 960/1000, Training Loss: 7.09265661239624, Validation Loss: 7.1203932762146\n",
      "Epoch: 961/1000, Training Loss: 7.092652797698975, Validation Loss: 7.120390892028809\n",
      "Epoch: 962/1000, Training Loss: 7.092648506164551, Validation Loss: 7.120388507843018\n",
      "Epoch: 963/1000, Training Loss: 7.092644214630127, Validation Loss: 7.120387077331543\n",
      "Epoch: 964/1000, Training Loss: 7.092640399932861, Validation Loss: 7.120384693145752\n",
      "Epoch: 965/1000, Training Loss: 7.092636585235596, Validation Loss: 7.120382308959961\n",
      "Epoch: 966/1000, Training Loss: 7.092631816864014, Validation Loss: 7.12037992477417\n",
      "Epoch: 967/1000, Training Loss: 7.092628479003906, Validation Loss: 7.1203789710998535\n",
      "Epoch: 968/1000, Training Loss: 7.092624187469482, Validation Loss: 7.120375633239746\n",
      "Epoch: 969/1000, Training Loss: 7.092619895935059, Validation Loss: 7.1203742027282715\n",
      "Epoch: 970/1000, Training Loss: 7.092616558074951, Validation Loss: 7.120372772216797\n",
      "Epoch: 971/1000, Training Loss: 7.092613697052002, Validation Loss: 7.120369911193848\n",
      "Epoch: 972/1000, Training Loss: 7.09260892868042, Validation Loss: 7.120368480682373\n",
      "Epoch: 973/1000, Training Loss: 7.0926055908203125, Validation Loss: 7.120366096496582\n",
      "Epoch: 974/1000, Training Loss: 7.092601299285889, Validation Loss: 7.120365142822266\n",
      "Epoch: 975/1000, Training Loss: 7.092597961425781, Validation Loss: 7.120362281799316\n",
      "Epoch: 976/1000, Training Loss: 7.092594146728516, Validation Loss: 7.120359897613525\n",
      "Epoch: 977/1000, Training Loss: 7.092589378356934, Validation Loss: 7.120358467102051\n",
      "Epoch: 978/1000, Training Loss: 7.092586517333984, Validation Loss: 7.120355606079102\n",
      "Epoch: 979/1000, Training Loss: 7.0925822257995605, Validation Loss: 7.120354652404785\n",
      "Epoch: 980/1000, Training Loss: 7.092578887939453, Validation Loss: 7.120352745056152\n",
      "Epoch: 981/1000, Training Loss: 7.0925750732421875, Validation Loss: 7.120351314544678\n",
      "Epoch: 982/1000, Training Loss: 7.092571258544922, Validation Loss: 7.120348930358887\n",
      "Epoch: 983/1000, Training Loss: 7.0925679206848145, Validation Loss: 7.120347499847412\n",
      "Epoch: 984/1000, Training Loss: 7.092564105987549, Validation Loss: 7.120345115661621\n",
      "Epoch: 985/1000, Training Loss: 7.092559814453125, Validation Loss: 7.1203436851501465\n",
      "Epoch: 986/1000, Training Loss: 7.092556476593018, Validation Loss: 7.120341777801514\n",
      "Epoch: 987/1000, Training Loss: 7.09255313873291, Validation Loss: 7.120339393615723\n",
      "Epoch: 988/1000, Training Loss: 7.0925493240356445, Validation Loss: 7.120337963104248\n",
      "Epoch: 989/1000, Training Loss: 7.092545986175537, Validation Loss: 7.120336055755615\n",
      "Epoch: 990/1000, Training Loss: 7.0925421714782715, Validation Loss: 7.120334148406982\n",
      "Epoch: 991/1000, Training Loss: 7.092538833618164, Validation Loss: 7.120332717895508\n",
      "Epoch: 992/1000, Training Loss: 7.09253454208374, Validation Loss: 7.120330810546875\n",
      "Epoch: 993/1000, Training Loss: 7.092531681060791, Validation Loss: 7.120328903198242\n",
      "Epoch: 994/1000, Training Loss: 7.092527389526367, Validation Loss: 7.120327472686768\n",
      "Epoch: 995/1000, Training Loss: 7.092525005340576, Validation Loss: 7.120325565338135\n",
      "Epoch: 996/1000, Training Loss: 7.092520713806152, Validation Loss: 7.12032413482666\n",
      "Epoch: 997/1000, Training Loss: 7.092517852783203, Validation Loss: 7.120322227478027\n",
      "Epoch: 998/1000, Training Loss: 7.092514514923096, Validation Loss: 7.120319843292236\n",
      "Epoch: 999/1000, Training Loss: 7.092511177062988, Validation Loss: 7.120318412780762\n",
      "Epoch: 1000/1000, Training Loss: 7.0925068855285645, Validation Loss: 7.120316982269287\n",
      "Best epoch:  393\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "accuracies_train = []\n",
    "accuracies_val = []\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    losses_train.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # accuracy\n",
    "    with torch.no_grad():\n",
    "        predicted = (outputs >= 0.5).squeeze().long()\n",
    "        accuracy = accuracy_score(y_train, predicted)\n",
    "        accuracies_train.append(accuracy)\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_outputs = model(X_val)\n",
    "\n",
    "    # val loss\n",
    "    val_loss = criterion(val_outputs, y_val)\n",
    "    losses_val.append(val_loss)\n",
    "\n",
    "    # val accuracy\n",
    "    with torch.no_grad():\n",
    "        predicted_val = (val_outputs >= 0.5).squeeze().long() \n",
    "        accuracy_val = accuracy_score(y_val, predicted_val)\n",
    "        accuracies_val.append(accuracy_val)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "        best_weights = model.state_dict()\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "print(\"Best epoch: \", best_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to tensors\n",
    "losses_train = torch.tensor(losses_train)\n",
    "losses_val = torch.tensor(losses_val)\n",
    "accuracies_train = torch.tensor(accuracies_train)\n",
    "accuracies_val = torch.tensor(accuracies_val)\n",
    "\n",
    "# training and validation losses\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_train.detach().numpy(), label='Training Loss')\n",
    "plt.plot(losses_val.detach().numpy(), label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies_train.detach().numpy(), label='Training Accuracy')\n",
    "plt.plot(accuracies_val.detach().numpy(), label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики на тестових даних"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Метрики*:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall \n",
    "- f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = model(X_test)\n",
    "test_outputs = (test_outputs >= 0.5).squeeze().long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(y_test, test_outputs)\n",
    "test_precision = precision_score(y_test, test_outputs)\n",
    "test_recall = recall_score(y_test, test_outputs)\n",
    "test_f1 = f1_score(y_test, test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9218838127467569\n",
      "Precision: 0.9065520945220193\n",
      "Recall: 0.9424902289223898\n",
      "f1: 0.9241719134957569\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", test_accuracy)\n",
    "print(\"Precision:\", test_precision)\n",
    "print(\"Recall:\", test_recall)\n",
    "print(\"f1:\", test_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
